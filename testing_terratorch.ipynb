{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import torch \n",
    "import terratorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}\n",
    "print(test.get('img_prefix'))\n",
    "\n",
    "from osgeo import gdal\n",
    "import rioxarray\n",
    "testfile = r'C:\\Users\\timvd\\Documents\\Uni_2024-2025\\UC\\Project\\ProjectCode\\data\\images\\images\\training\\120x120\\positive\\198_2019-02-15T10_06_39.586Z_2.tif'\n",
    "info = gdal.Info(testfile)\n",
    "print(info)\n",
    "\n",
    "# def open_tiff(fname):\n",
    "#     data = rioxarray.open_rasterio(fname)\n",
    "#     return data.to_numpy()\n",
    "\n",
    "# open_tiff(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "\n",
    "from torchgeo.datasets import RasterDataset, stack_samples, unbind_samples,GeoDataset\n",
    "from torchgeo.datasets.utils import download_url\n",
    "from torchgeo.samplers import RandomGeoSampler,GeoSampler,RandomBatchGeoSampler\n",
    "\n",
    "import re\n",
    "from typing import cast\n",
    " \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "\n",
    "class Sentinel2(RasterDataset):\n",
    "    filename_glob = '*.tif'\n",
    "    #198_2019-01-31T10_06_36.654Z_1.tif\n",
    "    # filename_regex = r'^.{6}_(?P<date>\\d{8}T\\d{6})_(?P<band>B0[\\d])'\n",
    "    # date_format = '%Y%m%dT%H%M%S'\n",
    "    is_image = True\n",
    "    separate_files = True\n",
    "    all_bands = tuple([f'B0{i}' for i in range(1,14)])\n",
    "    rgb_bands = ('B04', 'B03', 'B02')\n",
    "\n",
    "    def plot(self, sample):\n",
    "        # Find the correct band index order\n",
    "        rgb_indices = []\n",
    "        for band in self.rgb_bands:\n",
    "            rgb_indices.append(self.all_bands.index(band))\n",
    "\n",
    "        # Reorder and rescale the image\n",
    "        image = sample['image'][rgb_indices].permute(1, 2, 0)\n",
    "        image = torch.clamp(image / 10000, min=0, max=1).numpy()\n",
    "\n",
    "        # Plot the image\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def __getitem__(self,query):\n",
    "        #dit is de source code van RasterDataSet.__getitem__() tot waar ik sample['test'] toevoeg\n",
    "        #kan een work-around zijn?\n",
    "        hits = self.index.intersection(tuple(query), objects=True)\n",
    "        filepaths = cast(list[str], [hit.object for hit in hits])\n",
    "\n",
    "        if not filepaths:\n",
    "            raise IndexError(\n",
    "                f'query: {query} not found in index with bounds: {self.bounds}'\n",
    "            )\n",
    "\n",
    "        if self.separate_files:\n",
    "            data_list: list[torch.Tensor] = []\n",
    "            filename_regex = re.compile(self.filename_regex, re.VERBOSE)\n",
    "            for band in self.bands:\n",
    "                band_filepaths = []\n",
    "                for filepath in filepaths:\n",
    "                    filename = os.path.basename(filepath)\n",
    "                    directory = os.path.dirname(filepath)\n",
    "                    match = re.match(filename_regex, filename)\n",
    "                    if match:\n",
    "                        if 'band' in match.groupdict():\n",
    "                            start = match.start('band')\n",
    "                            end = match.end('band')\n",
    "                            filename = filename[:start] + band + filename[end:]\n",
    "                    filepath = os.path.join(directory, filename)\n",
    "                    band_filepaths.append(filepath)\n",
    "                data_list.append(self._merge_files(band_filepaths, query))\n",
    "            data = torch.cat(data_list)\n",
    "        else:\n",
    "            data = self._merge_files(filepaths, query, self.band_indexes)\n",
    "\n",
    "        sample = {'crs': self.crs, 'bounds': query}\n",
    "\n",
    "        data = data.to(self.dtype)\n",
    "        if self.is_image:\n",
    "            sample['image'] = data\n",
    "        else:\n",
    "            sample['mask'] = data\n",
    "\n",
    "\n",
    "        #hier evt eigen dingen toevoegen aan de sample\n",
    "        sample['test'] = 1\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentinel2 Dataset\n",
      "    type: GeoDataset\n",
      "    bbox: BoundingBox(minx=-1585915.5339009934, maxx=1057978.5593479783, miny=4600624.042363878, maxy=6942656.59778398, mint=0.0, maxt=9.223372036854776e+18)\n",
      "    size: 560\n"
     ]
    }
   ],
   "source": [
    "root = r'C:\\Users\\timvd\\Documents\\Uni_2024-2025\\UC\\Project\\ProjectCode\\data\\images\\images\\training\\120x120\\positive'\n",
    "\n",
    "dataset = Sentinel2(root)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataLoader in module torch.utils.data.dataloader:\n",
      "\n",
      "class DataLoader(typing.Generic)\n",
      " |  DataLoader(dataset: torch.utils.data.dataset.Dataset[+_T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~_T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '')\n",
      " |  \n",
      " |  Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
      " |  \n",
      " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      " |  iterable-style datasets with single- or multi-process loading, customizing\n",
      " |  loading order and optional automatic batching (collation) and memory pinning.\n",
      " |  \n",
      " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
      " |  \n",
      " |  Args:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
      " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
      " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
      " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
      " |          returns a batch of indices at a time. Mutually exclusive with\n",
      " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
      " |          and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. ``0`` means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (Callable, optional): merges a list of samples to form a\n",
      " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
      " |          map-style dataset.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      " |          into device/CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (Callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |      multiprocessing_context (str or multiprocessing.context.BaseContext, optional): If\n",
      " |          ``None``, the default `multiprocessing context`_ of your operating system will\n",
      " |          be used. (default: ``None``)\n",
      " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
      " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
      " |          ``base_seed`` for workers. (default: ``None``)\n",
      " |      prefetch_factor (int, optional, keyword-only arg): Number of batches loaded\n",
      " |          in advance by each worker. ``2`` means there will be a total of\n",
      " |          2 * num_workers batches prefetched across all workers. (default value depends\n",
      " |          on the set value for num_workers. If value of num_workers=0 default is ``None``.\n",
      " |          Otherwise, if value of ``num_workers > 0`` default is ``2``).\n",
      " |      persistent_workers (bool, optional): If ``True``, the data loader will not shut down\n",
      " |          the worker processes after a dataset has been consumed once. This allows to\n",
      " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
      " |      pin_memory_device (str, optional): the device to :attr:`pin_memory` to if ``pin_memory`` is\n",
      " |          ``True``.\n",
      " |  \n",
      " |  \n",
      " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
      " |               :ref:`multiprocessing-best-practices` on more details related\n",
      " |               to multiprocessing in PyTorch.\n",
      " |  \n",
      " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
      " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
      " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
      " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
      " |               loading to avoid duplicate data.\n",
      " |  \n",
      " |               However, if sharding results in multiple workers having incomplete last batches,\n",
      " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
      " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
      " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
      " |               cases in general.\n",
      " |  \n",
      " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
      " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
      " |               `Multi-process data loading`_.\n",
      " |  \n",
      " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
      " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
      " |  \n",
      " |  .. _multiprocessing context:\n",
      " |      https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataLoader\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+_T_co], batch_size: Optional[int] = 1, shuffle: Optional[bool] = None, sampler: Union[torch.utils.data.sampler.Sampler, Iterable, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[List], Iterable[List], NoneType] = None, num_workers: int = 0, collate_fn: Optional[Callable[[List[~_T]], Any]] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Optional[Callable[[int], NoneType]] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: Optional[int] = None, persistent_workers: bool = False, pin_memory_device: str = '')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
      " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
      " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |  \n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  check_worker_number_rationality(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  multiprocessing_context\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_iterator': typing.Optional[ForwardRef('_BaseDataL...\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+_T_co],)\n",
      " |  \n",
      " |  __parameters__ = (+_T_co,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params)\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs)\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "# root = r'C:\\Users\\timvd\\Documents\\Uni_2024-2025\\UC\\Project\\ProjectCode\\data\\images\\images\\training\\120x120\\positive'\n",
    "# dataset = Sentinel2(root)\n",
    "# print(dataset)    \n",
    "sampler = RandomGeoSampler(dataset,size=120,length=1) \n",
    "dataloader = DataLoader(dataset, sampler=sampler,collate_fn=stack_samples) \n",
    "\n",
    "#test with batch size and such\n",
    "# sampler = RandomGeoSampler(dataset,size=120,length=5) #use length as a batch_size?\n",
    "# dataloader = DataLoader(dataset,batch_size=5, sampler=sampler,collate_fn=stack_samples) \n",
    "\n",
    "for batch in dataloader:\n",
    "    # print(batch)\n",
    "    sample = unbind_samples(batch)\n",
    "    print(len(sample))\n",
    "    # print(sample)\n",
    "    # dataset.plot(sample)\n",
    "    # plt.axis('off')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RasterDataset in module torchgeo.datasets.geo:\n",
      "\n",
      "class RasterDataset(GeoDataset)\n",
      " |  RasterDataset(paths: str | os.PathLike[str] | collections.abc.Iterable[str | os.PathLike[str]] = 'data', crs: rasterio.crs.CRS | None = None, res: float | None = None, bands: collections.abc.Sequence[str] | None = None, transforms: collections.abc.Callable[[dict[str, typing.Any]], dict[str, typing.Any]] | None = None, cache: bool = True) -> None\n",
      " |  \n",
      " |  Abstract base class for :class:`GeoDataset` stored as raster files.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RasterDataset\n",
      " |      GeoDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, query: torchgeo.datasets.utils.BoundingBox) -> dict[str, typing.Any]\n",
      " |      Retrieve image/mask and metadata indexed by query.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: (minx, maxx, miny, maxy, mint, maxt) coordinates to index\n",
      " |      \n",
      " |      Returns:\n",
      " |          sample of image/mask and metadata at that index\n",
      " |      \n",
      " |      Raises:\n",
      " |          IndexError: if query is not found in the index\n",
      " |  \n",
      " |  __init__(self, paths: str | os.PathLike[str] | collections.abc.Iterable[str | os.PathLike[str]] = 'data', crs: rasterio.crs.CRS | None = None, res: float | None = None, bands: collections.abc.Sequence[str] | None = None, transforms: collections.abc.Callable[[dict[str, typing.Any]], dict[str, typing.Any]] | None = None, cache: bool = True) -> None\n",
      " |      Initialize a new RasterDataset instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          paths: one or more root directories to search or files to load\n",
      " |          crs: :term:`coordinate reference system (CRS)` to warp to\n",
      " |              (defaults to the CRS of the first file found)\n",
      " |          res: resolution of the dataset in units of CRS\n",
      " |              (defaults to the resolution of the first file found)\n",
      " |          bands: bands to return (defaults to all bands)\n",
      " |          transforms: a function/transform that takes an input sample\n",
      " |              and returns a transformed version\n",
      " |          cache: if True, cache file handle to speed up repeated sampling\n",
      " |      \n",
      " |      Raises:\n",
      " |          DatasetNotFoundError: If dataset is not found.\n",
      " |      \n",
      " |      .. versionchanged:: 0.5\n",
      " |         *root* was renamed to *paths*.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the dataset (overrides the dtype of the data file via a cast).\n",
      " |      \n",
      " |      Defaults to float32 if :attr:`~RasterDataset.is_image` is True, else long.\n",
      " |      Can be overridden for tasks like pixel-wise regression where the mask should be\n",
      " |      float32 instead of long.\n",
      " |      \n",
      " |      Returns:\n",
      " |          the dtype of the dataset\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  resampling\n",
      " |      Resampling algorithm used when reading input files.\n",
      " |      \n",
      " |      Defaults to bilinear for float dtypes and nearest for int dtypes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The resampling method to use.\n",
      " |      \n",
      " |      .. versionadded:: 0.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'all_bands': tuple[str, ...], 'cmap': typing.ClassV...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  all_bands = ()\n",
      " |  \n",
      " |  cmap = {}\n",
      " |  \n",
      " |  date_format = '%Y%m%d'\n",
      " |  \n",
      " |  filename_regex = '.*'\n",
      " |  \n",
      " |  is_image = True\n",
      " |  \n",
      " |  maxt = 9223372036854775807\n",
      " |  \n",
      " |  mint = 0\n",
      " |  \n",
      " |  rgb_bands = ()\n",
      " |  \n",
      " |  separate_files = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from GeoDataset:\n",
      " |  \n",
      " |  __and__(self, other: 'GeoDataset') -> 'IntersectionDataset'\n",
      " |      Take the intersection of two :class:`GeoDataset`.\n",
      " |      \n",
      " |      Args:\n",
      " |          other: another dataset\n",
      " |      \n",
      " |      Returns:\n",
      " |          a single dataset\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if other is not a :class:`GeoDataset`\n",
      " |      \n",
      " |      .. versionadded:: 0.2\n",
      " |  \n",
      " |  __getstate__(self) -> tuple[dict[str, typing.Any], list[tuple[typing.Any, typing.Any, typing.Any | None]]]\n",
      " |      Define how instances are pickled.\n",
      " |      \n",
      " |      Returns:\n",
      " |          the state necessary to unpickle the instance\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Return the number of files in the dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          length of the dataset\n",
      " |  \n",
      " |  __or__(self, other: 'GeoDataset') -> 'UnionDataset'\n",
      " |      Take the union of two GeoDatasets.\n",
      " |      \n",
      " |      Args:\n",
      " |          other: another dataset\n",
      " |      \n",
      " |      Returns:\n",
      " |          a single dataset\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if other is not a :class:`GeoDataset`\n",
      " |      \n",
      " |      .. versionadded:: 0.2\n",
      " |  \n",
      " |  __setstate__(self, state: tuple[dict[typing.Any, typing.Any], list[tuple[int, tuple[float, float, float, float, float, float], str | os.PathLike[str]]]]) -> None\n",
      " |      Define how to unpickle an instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          state: the state of the instance when it was pickled\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return the informal string representation of the object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          informal string representation\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from GeoDataset:\n",
      " |  \n",
      " |  bounds\n",
      " |      Bounds of the index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          (minx, maxx, miny, maxy, mint, maxt) of the dataset\n",
      " |  \n",
      " |  files\n",
      " |      A list of all files in the dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          All files in the dataset.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from GeoDataset:\n",
      " |  \n",
      " |  crs\n",
      " |      :term:`coordinate reference system (CRS)` of the dataset.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The :term:`coordinate reference system (CRS)`.\n",
      " |  \n",
      " |  res\n",
      " |      Resolution of the dataset in units of CRS.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The resolution of the dataset.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from GeoDataset:\n",
      " |  \n",
      " |  __add__ = None\n",
      " |  \n",
      " |  __orig_bases__ = (torch.utils.data.dataset.Dataset[dict[str, typing.An...\n",
      " |  \n",
      " |  filename_glob = '*'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params)\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs)\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RasterDataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UC-env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
