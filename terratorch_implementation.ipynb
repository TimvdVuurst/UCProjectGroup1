{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "import terratorch\n",
    "from terratorch.tasks import ClassificationTask, PixelwiseRegressionTask\n",
    "\n",
    "from torchgeo.datasets import RasterDataset, stack_samples, unbind_samples, GeoDataset, UnionDataset\n",
    "from torchgeo.datasets.utils import download_url\n",
    "from torchgeo.samplers import RandomGeoSampler,GeoSampler,RandomBatchGeoSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model bands not passed. Assuming bands are ordered in the same way as [<HLSBands.BLUE: 'BLUE'>, <HLSBands.GREEN: 'GREEN'>, <HLSBands.RED: 'RED'>, <HLSBands.NIR_NARROW: 'NIR_NARROW'>, <HLSBands.SWIR_1: 'SWIR_1'>, <HLSBands.SWIR_2: 'SWIR_2'>].            Pretrained patch_embed layer may be misaligned with current bands\n",
      "INFO:timm.models._builder:Loading pretrained weights from file (Prithvi_EO_V1_100M.pt)\n",
      "INFO:timm.models._helpers:Loaded  from checkpoint 'Prithvi_EO_V1_100M.pt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalViTEncoder(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv3d(6, 768, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): Identity()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): Identity()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (decoder_pred): Linear(in_features=512, out_features=1536, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = timm.create_model(\n",
    "#         \"prithvi_vit_100\",\n",
    "#         pretrained=True,\n",
    "#         num_frames=num_frames,\n",
    "#         patch_size=patch_size,\n",
    "#         tubelet_size=tubelet_size,\n",
    "#         features_only=True,\n",
    "#     )\n",
    "\n",
    "# Example of how to create a model using timm \n",
    "model = timm.create_model(\n",
    "    \"prithvi_vit_100\", pretrained_cfg=dict(file=\"Prithvi_EO_V1_100M.pt\"), pretrained=True, num_classes=0, in_chans=6,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bat_resnext26ts.ch_in1k', 'beit_base_patch16_224.in22k_ft_in22k', 'beit_base_patch16_224.in22k_ft_in22k_in1k', 'beit_base_patch16_384.in22k_ft_in22k_in1k', 'beit_large_patch16_224.in22k_ft_in22k', 'beit_large_patch16_224.in22k_ft_in22k_in1k', 'beit_large_patch16_384.in22k_ft_in22k_in1k', 'beit_large_patch16_512.in22k_ft_in22k_in1k', 'beitv2_base_patch16_224.in1k_ft_in1k', 'beitv2_base_patch16_224.in1k_ft_in22k', 'beitv2_base_patch16_224.in1k_ft_in22k_in1k', 'beitv2_large_patch16_224.in1k_ft_in1k', 'beitv2_large_patch16_224.in1k_ft_in22k', 'beitv2_large_patch16_224.in1k_ft_in22k_in1k', 'botnet26t_256.c1_in1k', 'caformer_b36.sail_in1k', 'caformer_b36.sail_in1k_384', 'caformer_b36.sail_in22k', 'caformer_b36.sail_in22k_ft_in1k', 'caformer_b36.sail_in22k_ft_in1k_384', 'caformer_m36.sail_in1k', 'caformer_m36.sail_in1k_384', 'caformer_m36.sail_in22k', 'caformer_m36.sail_in22k_ft_in1k', 'caformer_m36.sail_in22k_ft_in1k_384', 'caformer_s18.sail_in1k', 'caformer_s18.sail_in1k_384', 'caformer_s18.sail_in22k', 'caformer_s18.sail_in22k_ft_in1k', 'caformer_s18.sail_in22k_ft_in1k_384', 'caformer_s36.sail_in1k', 'caformer_s36.sail_in1k_384', 'caformer_s36.sail_in22k', 'caformer_s36.sail_in22k_ft_in1k', 'caformer_s36.sail_in22k_ft_in1k_384', 'cait_m36_384.fb_dist_in1k', 'cait_m48_448.fb_dist_in1k', 'cait_s24_224.fb_dist_in1k', 'cait_s24_384.fb_dist_in1k', 'cait_s36_384.fb_dist_in1k', 'cait_xs24_384.fb_dist_in1k', 'cait_xxs24_224.fb_dist_in1k', 'cait_xxs24_384.fb_dist_in1k', 'cait_xxs36_224.fb_dist_in1k', 'cait_xxs36_384.fb_dist_in1k', 'coat_lite_medium.in1k', 'coat_lite_medium_384.in1k', 'coat_lite_mini.in1k', 'coat_lite_small.in1k', 'coat_lite_tiny.in1k', 'coat_mini.in1k', 'coat_small.in1k', 'coat_tiny.in1k', 'coatnet_0_rw_224.sw_in1k', 'coatnet_1_rw_224.sw_in1k', 'coatnet_2_rw_224.sw_in12k', 'coatnet_2_rw_224.sw_in12k_ft_in1k', 'coatnet_3_rw_224.sw_in12k', 'coatnet_bn_0_rw_224.sw_in1k', 'coatnet_nano_rw_224.sw_in1k', 'coatnet_rmlp_1_rw2_224.sw_in12k', 'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k', 'coatnet_rmlp_1_rw_224.sw_in1k', 'coatnet_rmlp_2_rw_224.sw_in1k', 'coatnet_rmlp_2_rw_224.sw_in12k', 'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k', 'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k', 'coatnet_rmlp_nano_rw_224.sw_in1k', 'coatnext_nano_rw_224.sw_in1k', 'convformer_b36.sail_in1k', 'convformer_b36.sail_in1k_384', 'convformer_b36.sail_in22k', 'convformer_b36.sail_in22k_ft_in1k', 'convformer_b36.sail_in22k_ft_in1k_384', 'convformer_m36.sail_in1k', 'convformer_m36.sail_in1k_384', 'convformer_m36.sail_in22k', 'convformer_m36.sail_in22k_ft_in1k', 'convformer_m36.sail_in22k_ft_in1k_384', 'convformer_s18.sail_in1k', 'convformer_s18.sail_in1k_384', 'convformer_s18.sail_in22k', 'convformer_s18.sail_in22k_ft_in1k', 'convformer_s18.sail_in22k_ft_in1k_384', 'convformer_s36.sail_in1k', 'convformer_s36.sail_in1k_384', 'convformer_s36.sail_in22k', 'convformer_s36.sail_in22k_ft_in1k', 'convformer_s36.sail_in22k_ft_in1k_384', 'convit_base.fb_in1k', 'convit_small.fb_in1k', 'convit_tiny.fb_in1k', 'convmixer_768_32.in1k', 'convmixer_1024_20_ks9_p14.in1k', 'convmixer_1536_20.in1k', 'convnext_atto.d2_in1k', 'convnext_atto_ols.a2_in1k', 'convnext_base.clip_laion2b', 'convnext_base.clip_laion2b_augreg', 'convnext_base.clip_laion2b_augreg_ft_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k', 'convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384', 'convnext_base.clip_laiona', 'convnext_base.clip_laiona_320', 'convnext_base.clip_laiona_augreg_320', 'convnext_base.clip_laiona_augreg_ft_in1k_384', 'convnext_base.fb_in1k', 'convnext_base.fb_in22k', 'convnext_base.fb_in22k_ft_in1k', 'convnext_base.fb_in22k_ft_in1k_384', 'convnext_femto.d1_in1k', 'convnext_femto_ols.d1_in1k', 'convnext_large.fb_in1k', 'convnext_large.fb_in22k', 'convnext_large.fb_in22k_ft_in1k', 'convnext_large.fb_in22k_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k', 'convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384', 'convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_ft_320', 'convnext_large_mlp.clip_laion2b_ft_soup_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_384', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320', 'convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384', 'convnext_nano.d1h_in1k', 'convnext_nano.in12k', 'convnext_nano.in12k_ft_in1k', 'convnext_nano_ols.d1h_in1k', 'convnext_pico.d1_in1k', 'convnext_pico_ols.d1_in1k', 'convnext_small.fb_in1k', 'convnext_small.fb_in22k', 'convnext_small.fb_in22k_ft_in1k', 'convnext_small.fb_in22k_ft_in1k_384', 'convnext_small.in12k', 'convnext_small.in12k_ft_in1k', 'convnext_small.in12k_ft_in1k_384', 'convnext_tiny.fb_in1k', 'convnext_tiny.fb_in22k', 'convnext_tiny.fb_in22k_ft_in1k', 'convnext_tiny.fb_in22k_ft_in1k_384', 'convnext_tiny.in12k', 'convnext_tiny.in12k_ft_in1k', 'convnext_tiny.in12k_ft_in1k_384', 'convnext_tiny_hnf.a2h_in1k', 'convnext_xlarge.fb_in22k', 'convnext_xlarge.fb_in22k_ft_in1k', 'convnext_xlarge.fb_in22k_ft_in1k_384', 'convnext_xxlarge.clip_laion2b_rewind', 'convnext_xxlarge.clip_laion2b_soup', 'convnext_xxlarge.clip_laion2b_soup_ft_in1k', 'convnextv2_atto.fcmae', 'convnextv2_atto.fcmae_ft_in1k', 'convnextv2_base.fcmae', 'convnextv2_base.fcmae_ft_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k', 'convnextv2_base.fcmae_ft_in22k_in1k_384', 'convnextv2_femto.fcmae', 'convnextv2_femto.fcmae_ft_in1k', 'convnextv2_huge.fcmae', 'convnextv2_huge.fcmae_ft_in1k', 'convnextv2_huge.fcmae_ft_in22k_in1k_384', 'convnextv2_huge.fcmae_ft_in22k_in1k_512', 'convnextv2_large.fcmae', 'convnextv2_large.fcmae_ft_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k', 'convnextv2_large.fcmae_ft_in22k_in1k_384', 'convnextv2_nano.fcmae', 'convnextv2_nano.fcmae_ft_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k', 'convnextv2_nano.fcmae_ft_in22k_in1k_384', 'convnextv2_pico.fcmae', 'convnextv2_pico.fcmae_ft_in1k', 'convnextv2_tiny.fcmae', 'convnextv2_tiny.fcmae_ft_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k', 'convnextv2_tiny.fcmae_ft_in22k_in1k_384', 'crossvit_9_240.in1k', 'crossvit_9_dagger_240.in1k', 'crossvit_15_240.in1k', 'crossvit_15_dagger_240.in1k', 'crossvit_15_dagger_408.in1k', 'crossvit_18_240.in1k', 'crossvit_18_dagger_240.in1k', 'crossvit_18_dagger_408.in1k', 'crossvit_base_240.in1k', 'crossvit_small_240.in1k', 'crossvit_tiny_240.in1k', 'cs3darknet_focus_l.c2ns_in1k', 'cs3darknet_focus_m.c2ns_in1k', 'cs3darknet_l.c2ns_in1k', 'cs3darknet_m.c2ns_in1k', 'cs3darknet_x.c2ns_in1k', 'cs3edgenet_x.c2_in1k', 'cs3se_edgenet_x.c2ns_in1k', 'cs3sedarknet_l.c2ns_in1k', 'cs3sedarknet_x.c2ns_in1k', 'cspdarknet53.ra_in1k', 'cspresnet50.ra_in1k', 'cspresnext50.ra_in1k', 'darknet53.c2ns_in1k', 'darknetaa53.c2ns_in1k', 'davit_base.msft_in1k', 'davit_small.msft_in1k', 'davit_tiny.msft_in1k', 'deit3_base_patch16_224.fb_in1k', 'deit3_base_patch16_224.fb_in22k_ft_in1k', 'deit3_base_patch16_384.fb_in1k', 'deit3_base_patch16_384.fb_in22k_ft_in1k', 'deit3_huge_patch14_224.fb_in1k', 'deit3_huge_patch14_224.fb_in22k_ft_in1k', 'deit3_large_patch16_224.fb_in1k', 'deit3_large_patch16_224.fb_in22k_ft_in1k', 'deit3_large_patch16_384.fb_in1k', 'deit3_large_patch16_384.fb_in22k_ft_in1k', 'deit3_medium_patch16_224.fb_in1k', 'deit3_medium_patch16_224.fb_in22k_ft_in1k', 'deit3_small_patch16_224.fb_in1k', 'deit3_small_patch16_224.fb_in22k_ft_in1k', 'deit3_small_patch16_384.fb_in1k', 'deit3_small_patch16_384.fb_in22k_ft_in1k', 'deit_base_distilled_patch16_224.fb_in1k', 'deit_base_distilled_patch16_384.fb_in1k', 'deit_base_patch16_224.fb_in1k', 'deit_base_patch16_384.fb_in1k', 'deit_small_distilled_patch16_224.fb_in1k', 'deit_small_patch16_224.fb_in1k', 'deit_tiny_distilled_patch16_224.fb_in1k', 'deit_tiny_patch16_224.fb_in1k', 'densenet121.ra_in1k', 'densenet121.tv_in1k', 'densenet161.tv_in1k', 'densenet169.tv_in1k', 'densenet201.tv_in1k', 'densenetblur121d.ra_in1k', 'dla34.in1k', 'dla46_c.in1k', 'dla46x_c.in1k', 'dla60.in1k', 'dla60_res2net.in1k', 'dla60_res2next.in1k', 'dla60x.in1k', 'dla60x_c.in1k', 'dla102.in1k', 'dla102x2.in1k', 'dla102x.in1k', 'dla169.in1k', 'dm_nfnet_f0.dm_in1k', 'dm_nfnet_f1.dm_in1k', 'dm_nfnet_f2.dm_in1k', 'dm_nfnet_f3.dm_in1k', 'dm_nfnet_f4.dm_in1k', 'dm_nfnet_f5.dm_in1k', 'dm_nfnet_f6.dm_in1k', 'dpn68.mx_in1k', 'dpn68b.mx_in1k', 'dpn68b.ra_in1k', 'dpn92.mx_in1k', 'dpn98.mx_in1k', 'dpn107.mx_in1k', 'dpn131.mx_in1k', 'eca_botnext26ts_256.c1_in1k', 'eca_halonext26ts.c1_in1k', 'eca_nfnet_l0.ra2_in1k', 'eca_nfnet_l1.ra2_in1k', 'eca_nfnet_l2.ra3_in1k', 'eca_resnet33ts.ra2_in1k', 'eca_resnext26ts.ch_in1k', 'ecaresnet26t.ra2_in1k', 'ecaresnet50d.miil_in1k', 'ecaresnet50d_pruned.miil_in1k', 'ecaresnet50t.a1_in1k', 'ecaresnet50t.a2_in1k', 'ecaresnet50t.a3_in1k', 'ecaresnet50t.ra2_in1k', 'ecaresnet101d.miil_in1k', 'ecaresnet101d_pruned.miil_in1k', 'ecaresnet269d.ra2_in1k', 'ecaresnetlight.miil_in1k', 'edgenext_base.in21k_ft_in1k', 'edgenext_base.usi_in1k', 'edgenext_small.usi_in1k', 'edgenext_small_rw.sw_in1k', 'edgenext_x_small.in1k', 'edgenext_xx_small.in1k', 'efficientformer_l1.snap_dist_in1k', 'efficientformer_l3.snap_dist_in1k', 'efficientformer_l7.snap_dist_in1k', 'efficientformerv2_l.snap_dist_in1k', 'efficientformerv2_s0.snap_dist_in1k', 'efficientformerv2_s1.snap_dist_in1k', 'efficientformerv2_s2.snap_dist_in1k', 'efficientnet_b0.ra_in1k', 'efficientnet_b1.ft_in1k', 'efficientnet_b1_pruned.in1k', 'efficientnet_b2.ra_in1k', 'efficientnet_b2_pruned.in1k', 'efficientnet_b3.ra2_in1k', 'efficientnet_b3_pruned.in1k', 'efficientnet_b4.ra2_in1k', 'efficientnet_b5.sw_in12k', 'efficientnet_b5.sw_in12k_ft_in1k', 'efficientnet_el.ra_in1k', 'efficientnet_el_pruned.in1k', 'efficientnet_em.ra2_in1k', 'efficientnet_es.ra_in1k', 'efficientnet_es_pruned.in1k', 'efficientnet_lite0.ra_in1k', 'efficientnetv2_rw_m.agc_in1k', 'efficientnetv2_rw_s.ra2_in1k', 'efficientnetv2_rw_t.ra2_in1k', 'ese_vovnet19b_dw.ra_in1k', 'ese_vovnet39b.ra_in1k', 'eva02_base_patch14_224.mim_in22k', 'eva02_base_patch14_448.mim_in22k_ft_in1k', 'eva02_base_patch14_448.mim_in22k_ft_in22k', 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k', 'eva02_base_patch16_clip_224.merged2b', 'eva02_enormous_patch14_clip_224.laion2b', 'eva02_enormous_patch14_clip_224.laion2b_plus', 'eva02_large_patch14_224.mim_in22k', 'eva02_large_patch14_224.mim_m38m', 'eva02_large_patch14_448.mim_in22k_ft_in1k', 'eva02_large_patch14_448.mim_in22k_ft_in22k', 'eva02_large_patch14_448.mim_in22k_ft_in22k_in1k', 'eva02_large_patch14_448.mim_m38m_ft_in1k', 'eva02_large_patch14_448.mim_m38m_ft_in22k', 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', 'eva02_large_patch14_clip_224.merged2b', 'eva02_large_patch14_clip_336.merged2b', 'eva02_small_patch14_224.mim_in22k', 'eva02_small_patch14_336.mim_in22k_ft_in1k', 'eva02_tiny_patch14_224.mim_in22k', 'eva02_tiny_patch14_336.mim_in22k_ft_in1k', 'eva_giant_patch14_224.clip_ft_in1k', 'eva_giant_patch14_336.clip_ft_in1k', 'eva_giant_patch14_336.m30m_ft_in22k_in1k', 'eva_giant_patch14_560.m30m_ft_in22k_in1k', 'eva_giant_patch14_clip_224.laion400m', 'eva_giant_patch14_clip_224.merged2b', 'eva_large_patch14_196.in22k_ft_in1k', 'eva_large_patch14_196.in22k_ft_in22k_in1k', 'eva_large_patch14_336.in22k_ft_in1k', 'eva_large_patch14_336.in22k_ft_in22k_in1k', 'fbnetc_100.rmsp_in1k', 'fbnetv3_b.ra2_in1k', 'fbnetv3_d.ra2_in1k', 'fbnetv3_g.ra2_in1k', 'flexivit_base.300ep_in1k', 'flexivit_base.300ep_in21k', 'flexivit_base.600ep_in1k', 'flexivit_base.1000ep_in21k', 'flexivit_base.1200ep_in1k', 'flexivit_base.patch16_in21k', 'flexivit_base.patch30_in21k', 'flexivit_large.300ep_in1k', 'flexivit_large.600ep_in1k', 'flexivit_large.1200ep_in1k', 'flexivit_small.300ep_in1k', 'flexivit_small.600ep_in1k', 'flexivit_small.1200ep_in1k', 'focalnet_base_lrf.ms_in1k', 'focalnet_base_srf.ms_in1k', 'focalnet_huge_fl3.ms_in22k', 'focalnet_huge_fl4.ms_in22k', 'focalnet_large_fl3.ms_in22k', 'focalnet_large_fl4.ms_in22k', 'focalnet_small_lrf.ms_in1k', 'focalnet_small_srf.ms_in1k', 'focalnet_tiny_lrf.ms_in1k', 'focalnet_tiny_srf.ms_in1k', 'focalnet_xlarge_fl3.ms_in22k', 'focalnet_xlarge_fl4.ms_in22k', 'gc_efficientnetv2_rw_t.agc_in1k', 'gcresnet33ts.ra2_in1k', 'gcresnet50t.ra2_in1k', 'gcresnext26ts.ch_in1k', 'gcresnext50ts.ch_in1k', 'gcvit_base.in1k', 'gcvit_small.in1k', 'gcvit_tiny.in1k', 'gcvit_xtiny.in1k', 'gcvit_xxtiny.in1k', 'gernet_l.idstcv_in1k', 'gernet_m.idstcv_in1k', 'gernet_s.idstcv_in1k', 'ghostnet_100.in1k', 'gmixer_24_224.ra3_in1k', 'gmlp_s16_224.ra3_in1k', 'halo2botnet50ts_256.a1h_in1k', 'halonet26t.a1h_in1k', 'halonet50ts.a1h_in1k', 'haloregnetz_b.ra3_in1k', 'hardcorenas_a.miil_green_in1k', 'hardcorenas_b.miil_green_in1k', 'hardcorenas_c.miil_green_in1k', 'hardcorenas_d.miil_green_in1k', 'hardcorenas_e.miil_green_in1k', 'hardcorenas_f.miil_green_in1k', 'hrnet_w18.ms_aug_in1k', 'hrnet_w18.ms_in1k', 'hrnet_w18_small.ms_in1k', 'hrnet_w18_small_v2.ms_in1k', 'hrnet_w18_ssld.paddle_in1k', 'hrnet_w30.ms_in1k', 'hrnet_w32.ms_in1k', 'hrnet_w40.ms_in1k', 'hrnet_w44.ms_in1k', 'hrnet_w48.ms_in1k', 'hrnet_w48_ssld.paddle_in1k', 'hrnet_w64.ms_in1k', 'inception_resnet_v2.tf_ens_adv_in1k', 'inception_resnet_v2.tf_in1k', 'inception_v3.gluon_in1k', 'inception_v3.tf_adv_in1k', 'inception_v3.tf_in1k', 'inception_v3.tv_in1k', 'inception_v4.tf_in1k', 'lambda_resnet26rpt_256.c1_in1k', 'lambda_resnet26t.c1_in1k', 'lambda_resnet50ts.a1h_in1k', 'lamhalobotnet50ts_256.a1h_in1k', 'lcnet_050.ra2_in1k', 'lcnet_075.ra2_in1k', 'lcnet_100.ra2_in1k', 'legacy_senet154.in1k', 'legacy_seresnet18.in1k', 'legacy_seresnet34.in1k', 'legacy_seresnet50.in1k', 'legacy_seresnet101.in1k', 'legacy_seresnet152.in1k', 'legacy_seresnext26_32x4d.in1k', 'legacy_seresnext50_32x4d.in1k', 'legacy_seresnext101_32x4d.in1k', 'legacy_xception.tf_in1k', 'levit_128.fb_dist_in1k', 'levit_128s.fb_dist_in1k', 'levit_192.fb_dist_in1k', 'levit_256.fb_dist_in1k', 'levit_384.fb_dist_in1k', 'levit_conv_128.fb_dist_in1k', 'levit_conv_128s.fb_dist_in1k', 'levit_conv_192.fb_dist_in1k', 'levit_conv_256.fb_dist_in1k', 'levit_conv_384.fb_dist_in1k', 'maxvit_base_tf_224.in1k', 'maxvit_base_tf_224.in21k', 'maxvit_base_tf_384.in1k', 'maxvit_base_tf_384.in21k_ft_in1k', 'maxvit_base_tf_512.in1k', 'maxvit_base_tf_512.in21k_ft_in1k', 'maxvit_large_tf_224.in1k', 'maxvit_large_tf_224.in21k', 'maxvit_large_tf_384.in1k', 'maxvit_large_tf_384.in21k_ft_in1k', 'maxvit_large_tf_512.in1k', 'maxvit_large_tf_512.in21k_ft_in1k', 'maxvit_nano_rw_256.sw_in1k', 'maxvit_rmlp_base_rw_224.sw_in12k', 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k', 'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k', 'maxvit_rmlp_nano_rw_256.sw_in1k', 'maxvit_rmlp_pico_rw_256.sw_in1k', 'maxvit_rmlp_small_rw_224.sw_in1k', 'maxvit_rmlp_tiny_rw_256.sw_in1k', 'maxvit_small_tf_224.in1k', 'maxvit_small_tf_384.in1k', 'maxvit_small_tf_512.in1k', 'maxvit_tiny_rw_224.sw_in1k', 'maxvit_tiny_tf_224.in1k', 'maxvit_tiny_tf_384.in1k', 'maxvit_tiny_tf_512.in1k', 'maxvit_xlarge_tf_224.in21k', 'maxvit_xlarge_tf_384.in21k_ft_in1k', 'maxvit_xlarge_tf_512.in21k_ft_in1k', 'maxxvit_rmlp_nano_rw_256.sw_in1k', 'maxxvit_rmlp_small_rw_256.sw_in1k', 'maxxvitv2_nano_rw_256.sw_in1k', 'maxxvitv2_rmlp_base_rw_224.sw_in12k', 'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k', 'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k', 'mixer_b16_224.goog_in21k', 'mixer_b16_224.goog_in21k_ft_in1k', 'mixer_b16_224.miil_in21k', 'mixer_b16_224.miil_in21k_ft_in1k', 'mixer_l16_224.goog_in21k', 'mixer_l16_224.goog_in21k_ft_in1k', 'mixnet_l.ft_in1k', 'mixnet_m.ft_in1k', 'mixnet_s.ft_in1k', 'mixnet_xl.ra_in1k', 'mnasnet_100.rmsp_in1k', 'mnasnet_small.lamb_in1k', 'mobilenetv2_050.lamb_in1k', 'mobilenetv2_100.ra_in1k', 'mobilenetv2_110d.ra_in1k', 'mobilenetv2_120d.ra_in1k', 'mobilenetv2_140.ra_in1k', 'mobilenetv3_large_100.miil_in21k', 'mobilenetv3_large_100.miil_in21k_ft_in1k', 'mobilenetv3_large_100.ra_in1k', 'mobilenetv3_rw.rmsp_in1k', 'mobilenetv3_small_050.lamb_in1k', 'mobilenetv3_small_075.lamb_in1k', 'mobilenetv3_small_100.lamb_in1k', 'mobilevit_s.cvnets_in1k', 'mobilevit_xs.cvnets_in1k', 'mobilevit_xxs.cvnets_in1k', 'mobilevitv2_050.cvnets_in1k', 'mobilevitv2_075.cvnets_in1k', 'mobilevitv2_100.cvnets_in1k', 'mobilevitv2_125.cvnets_in1k', 'mobilevitv2_150.cvnets_in1k', 'mobilevitv2_150.cvnets_in22k_ft_in1k', 'mobilevitv2_150.cvnets_in22k_ft_in1k_384', 'mobilevitv2_175.cvnets_in1k', 'mobilevitv2_175.cvnets_in22k_ft_in1k', 'mobilevitv2_175.cvnets_in22k_ft_in1k_384', 'mobilevitv2_200.cvnets_in1k', 'mobilevitv2_200.cvnets_in22k_ft_in1k', 'mobilevitv2_200.cvnets_in22k_ft_in1k_384', 'mvitv2_base.fb_in1k', 'mvitv2_base_cls.fb_inw21k', 'mvitv2_huge_cls.fb_inw21k', 'mvitv2_large.fb_in1k', 'mvitv2_large_cls.fb_inw21k', 'mvitv2_small.fb_in1k', 'mvitv2_tiny.fb_in1k', 'nasnetalarge.tf_in1k', 'nest_base_jx.goog_in1k', 'nest_small_jx.goog_in1k', 'nest_tiny_jx.goog_in1k', 'nf_regnet_b1.ra2_in1k', 'nf_resnet50.ra2_in1k', 'nfnet_l0.ra2_in1k', 'pit_b_224.in1k', 'pit_b_distilled_224.in1k', 'pit_s_224.in1k', 'pit_s_distilled_224.in1k', 'pit_ti_224.in1k', 'pit_ti_distilled_224.in1k', 'pit_xs_224.in1k', 'pit_xs_distilled_224.in1k', 'pnasnet5large.tf_in1k', 'poolformer_m36.sail_in1k', 'poolformer_m48.sail_in1k', 'poolformer_s12.sail_in1k', 'poolformer_s24.sail_in1k', 'poolformer_s36.sail_in1k', 'poolformerv2_m36.sail_in1k', 'poolformerv2_m48.sail_in1k', 'poolformerv2_s12.sail_in1k', 'poolformerv2_s24.sail_in1k', 'poolformerv2_s36.sail_in1k', 'prithvi_vit_100', 'pvt_v2_b0.in1k', 'pvt_v2_b1.in1k', 'pvt_v2_b2.in1k', 'pvt_v2_b2_li.in1k', 'pvt_v2_b3.in1k', 'pvt_v2_b4.in1k', 'pvt_v2_b5.in1k', 'regnetv_040.ra3_in1k', 'regnetv_064.ra3_in1k', 'regnetx_002.pycls_in1k', 'regnetx_004.pycls_in1k', 'regnetx_004_tv.tv2_in1k', 'regnetx_006.pycls_in1k', 'regnetx_008.pycls_in1k', 'regnetx_008.tv2_in1k', 'regnetx_016.pycls_in1k', 'regnetx_016.tv2_in1k', 'regnetx_032.pycls_in1k', 'regnetx_032.tv2_in1k', 'regnetx_040.pycls_in1k', 'regnetx_064.pycls_in1k', 'regnetx_080.pycls_in1k', 'regnetx_080.tv2_in1k', 'regnetx_120.pycls_in1k', 'regnetx_160.pycls_in1k', 'regnetx_160.tv2_in1k', 'regnetx_320.pycls_in1k', 'regnetx_320.tv2_in1k', 'regnety_002.pycls_in1k', 'regnety_004.pycls_in1k', 'regnety_004.tv2_in1k', 'regnety_006.pycls_in1k', 'regnety_008.pycls_in1k', 'regnety_008_tv.tv2_in1k', 'regnety_016.pycls_in1k', 'regnety_016.tv2_in1k', 'regnety_032.pycls_in1k', 'regnety_032.ra_in1k', 'regnety_032.tv2_in1k', 'regnety_040.pycls_in1k', 'regnety_040.ra3_in1k', 'regnety_064.pycls_in1k', 'regnety_064.ra3_in1k', 'regnety_080.pycls_in1k', 'regnety_080.ra3_in1k', 'regnety_080_tv.tv2_in1k', 'regnety_120.pycls_in1k', 'regnety_120.sw_in12k', 'regnety_120.sw_in12k_ft_in1k', 'regnety_160.deit_in1k', 'regnety_160.lion_in12k_ft_in1k', 'regnety_160.pycls_in1k', 'regnety_160.sw_in12k', 'regnety_160.sw_in12k_ft_in1k', 'regnety_160.swag_ft_in1k', 'regnety_160.swag_lc_in1k', 'regnety_160.tv2_in1k', 'regnety_320.pycls_in1k', 'regnety_320.seer', 'regnety_320.seer_ft_in1k', 'regnety_320.swag_ft_in1k', 'regnety_320.swag_lc_in1k', 'regnety_320.tv2_in1k', 'regnety_640.seer', 'regnety_640.seer_ft_in1k', 'regnety_1280.seer', 'regnety_1280.seer_ft_in1k', 'regnety_1280.swag_ft_in1k', 'regnety_1280.swag_lc_in1k', 'regnety_2560.seer_ft_in1k', 'regnetz_040.ra3_in1k', 'regnetz_040_h.ra3_in1k', 'regnetz_b16.ra3_in1k', 'regnetz_c16.ra3_in1k', 'regnetz_c16_evos.ch_in1k', 'regnetz_d8.ra3_in1k', 'regnetz_d8_evos.ch_in1k', 'regnetz_d32.ra3_in1k', 'regnetz_e8.ra3_in1k', 'repvgg_a2.rvgg_in1k', 'repvgg_b0.rvgg_in1k', 'repvgg_b1.rvgg_in1k', 'repvgg_b1g4.rvgg_in1k', 'repvgg_b2.rvgg_in1k', 'repvgg_b2g4.rvgg_in1k', 'repvgg_b3.rvgg_in1k', 'repvgg_b3g4.rvgg_in1k', 'res2net50_14w_8s.in1k', 'res2net50_26w_4s.in1k', 'res2net50_26w_6s.in1k', 'res2net50_26w_8s.in1k', 'res2net50_48w_2s.in1k', 'res2net50d.in1k', 'res2net101_26w_4s.in1k', 'res2net101d.in1k', 'res2next50.in1k', 'resmlp_12_224.fb_dino', 'resmlp_12_224.fb_distilled_in1k', 'resmlp_12_224.fb_in1k', 'resmlp_24_224.fb_dino', 'resmlp_24_224.fb_distilled_in1k', 'resmlp_24_224.fb_in1k', 'resmlp_36_224.fb_distilled_in1k', 'resmlp_36_224.fb_in1k', 'resmlp_big_24_224.fb_distilled_in1k', 'resmlp_big_24_224.fb_in1k', 'resmlp_big_24_224.fb_in22k_ft_in1k', 'resnest14d.gluon_in1k', 'resnest26d.gluon_in1k', 'resnest50d.in1k', 'resnest50d_1s4x24d.in1k', 'resnest50d_4s2x40d.in1k', 'resnest101e.in1k', 'resnest200e.in1k', 'resnest269e.in1k', 'resnet10t.c3_in1k', 'resnet14t.c3_in1k', 'resnet18.a1_in1k', 'resnet18.a2_in1k', 'resnet18.a3_in1k', 'resnet18.fb_ssl_yfcc100m_ft_in1k', 'resnet18.fb_swsl_ig1b_ft_in1k', 'resnet18.gluon_in1k', 'resnet18.tv_in1k', 'resnet18d.ra2_in1k', 'resnet26.bt_in1k', 'resnet26d.bt_in1k', 'resnet26t.ra2_in1k', 'resnet32ts.ra2_in1k', 'resnet33ts.ra2_in1k', 'resnet34.a1_in1k', 'resnet34.a2_in1k', 'resnet34.a3_in1k', 'resnet34.bt_in1k', 'resnet34.gluon_in1k', 'resnet34.tv_in1k', 'resnet34d.ra2_in1k', 'resnet50.a1_in1k', 'resnet50.a1h_in1k', 'resnet50.a2_in1k', 'resnet50.a3_in1k', 'resnet50.am_in1k', 'resnet50.b1k_in1k', 'resnet50.b2k_in1k', 'resnet50.bt_in1k', 'resnet50.c1_in1k', 'resnet50.c2_in1k', 'resnet50.d_in1k', 'resnet50.fb_ssl_yfcc100m_ft_in1k', 'resnet50.fb_swsl_ig1b_ft_in1k', 'resnet50.gluon_in1k', 'resnet50.ra_in1k', 'resnet50.ram_in1k', 'resnet50.tv2_in1k', 'resnet50.tv_in1k', 'resnet50_gn.a1h_in1k', 'resnet50c.gluon_in1k', 'resnet50d.a1_in1k', 'resnet50d.a2_in1k', 'resnet50d.a3_in1k', 'resnet50d.gluon_in1k', 'resnet50d.ra2_in1k', 'resnet50s.gluon_in1k', 'resnet51q.ra2_in1k', 'resnet61q.ra2_in1k', 'resnet101.a1_in1k', 'resnet101.a1h_in1k', 'resnet101.a2_in1k', 'resnet101.a3_in1k', 'resnet101.gluon_in1k', 'resnet101.tv2_in1k', 'resnet101.tv_in1k', 'resnet101c.gluon_in1k', 'resnet101d.gluon_in1k', 'resnet101d.ra2_in1k', 'resnet101s.gluon_in1k', 'resnet152.a1_in1k', 'resnet152.a1h_in1k', 'resnet152.a2_in1k', 'resnet152.a3_in1k', 'resnet152.gluon_in1k', 'resnet152.tv2_in1k', 'resnet152.tv_in1k', 'resnet152c.gluon_in1k', 'resnet152d.gluon_in1k', 'resnet152d.ra2_in1k', 'resnet152s.gluon_in1k', 'resnet200d.ra2_in1k', 'resnetaa50.a1h_in1k', 'resnetaa50d.d_in12k', 'resnetaa50d.sw_in12k', 'resnetaa50d.sw_in12k_ft_in1k', 'resnetaa101d.sw_in12k', 'resnetaa101d.sw_in12k_ft_in1k', 'resnetblur50.bt_in1k', 'resnetrs50.tf_in1k', 'resnetrs101.tf_in1k', 'resnetrs152.tf_in1k', 'resnetrs200.tf_in1k', 'resnetrs270.tf_in1k', 'resnetrs350.tf_in1k', 'resnetrs420.tf_in1k', 'resnetv2_50.a1h_in1k', 'resnetv2_50d_evos.ah_in1k', 'resnetv2_50d_gn.ah_in1k', 'resnetv2_50x1_bit.goog_distilled_in1k', 'resnetv2_50x1_bit.goog_in21k', 'resnetv2_50x1_bit.goog_in21k_ft_in1k', 'resnetv2_50x3_bit.goog_in21k', 'resnetv2_50x3_bit.goog_in21k_ft_in1k', 'resnetv2_101.a1h_in1k', 'resnetv2_101x1_bit.goog_in21k', 'resnetv2_101x1_bit.goog_in21k_ft_in1k', 'resnetv2_101x3_bit.goog_in21k', 'resnetv2_101x3_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_in21k', 'resnetv2_152x2_bit.goog_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k', 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384', 'resnetv2_152x4_bit.goog_in21k', 'resnetv2_152x4_bit.goog_in21k_ft_in1k', 'resnext26ts.ra2_in1k', 'resnext50_32x4d.a1_in1k', 'resnext50_32x4d.a1h_in1k', 'resnext50_32x4d.a2_in1k', 'resnext50_32x4d.a3_in1k', 'resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k', 'resnext50_32x4d.fb_swsl_ig1b_ft_in1k', 'resnext50_32x4d.gluon_in1k', 'resnext50_32x4d.ra_in1k', 'resnext50_32x4d.tv2_in1k', 'resnext50_32x4d.tv_in1k', 'resnext50d_32x4d.bt_in1k', 'resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x4d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x4d.gluon_in1k', 'resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x8d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x8d.fb_wsl_ig1b_ft_in1k', 'resnext101_32x8d.tv2_in1k', 'resnext101_32x8d.tv_in1k', 'resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k', 'resnext101_32x16d.fb_swsl_ig1b_ft_in1k', 'resnext101_32x16d.fb_wsl_ig1b_ft_in1k', 'resnext101_32x32d.fb_wsl_ig1b_ft_in1k', 'resnext101_64x4d.c1_in1k', 'resnext101_64x4d.gluon_in1k', 'resnext101_64x4d.tv_in1k', 'rexnet_100.nav_in1k', 'rexnet_130.nav_in1k', 'rexnet_150.nav_in1k', 'rexnet_200.nav_in1k', 'rexnet_300.nav_in1k', 'rexnetr_200.sw_in12k', 'rexnetr_200.sw_in12k_ft_in1k', 'rexnetr_300.sw_in12k', 'rexnetr_300.sw_in12k_ft_in1k', 'sebotnet33ts_256.a1h_in1k', 'sehalonet33ts.ra2_in1k', 'SelecSls42b.in1k', 'SelecSls60.in1k', 'SelecSls60b.in1k', 'semnasnet_075.rmsp_in1k', 'semnasnet_100.rmsp_in1k', 'senet154.gluon_in1k', 'sequencer2d_l.in1k', 'sequencer2d_m.in1k', 'sequencer2d_s.in1k', 'seresnet33ts.ra2_in1k', 'seresnet50.a1_in1k', 'seresnet50.a2_in1k', 'seresnet50.a3_in1k', 'seresnet50.ra2_in1k', 'seresnet152d.ra2_in1k', 'seresnext26d_32x4d.bt_in1k', 'seresnext26t_32x4d.bt_in1k', 'seresnext26ts.ch_in1k', 'seresnext50_32x4d.gluon_in1k', 'seresnext50_32x4d.racm_in1k', 'seresnext101_32x4d.gluon_in1k', 'seresnext101_32x8d.ah_in1k', 'seresnext101_64x4d.gluon_in1k', 'seresnext101d_32x8d.ah_in1k', 'seresnextaa101d_32x8d.ah_in1k', 'seresnextaa101d_32x8d.sw_in12k', 'seresnextaa101d_32x8d.sw_in12k_ft_in1k', 'seresnextaa101d_32x8d.sw_in12k_ft_in1k_288', 'skresnet18.ra_in1k', 'skresnet34.ra_in1k', 'skresnext50_32x4d.ra_in1k', 'spnasnet_100.rmsp_in1k', 'swin_base_patch4_window7_224.ms_in1k', 'swin_base_patch4_window7_224.ms_in22k', 'swin_base_patch4_window7_224.ms_in22k_ft_in1k', 'swin_base_patch4_window12_384.ms_in1k', 'swin_base_patch4_window12_384.ms_in22k', 'swin_base_patch4_window12_384.ms_in22k_ft_in1k', 'swin_large_patch4_window7_224.ms_in22k', 'swin_large_patch4_window7_224.ms_in22k_ft_in1k', 'swin_large_patch4_window12_384.ms_in22k', 'swin_large_patch4_window12_384.ms_in22k_ft_in1k', 'swin_s3_base_224.ms_in1k', 'swin_s3_small_224.ms_in1k', 'swin_s3_tiny_224.ms_in1k', 'swin_small_patch4_window7_224.ms_in1k', 'swin_small_patch4_window7_224.ms_in22k', 'swin_small_patch4_window7_224.ms_in22k_ft_in1k', 'swin_tiny_patch4_window7_224.ms_in1k', 'swin_tiny_patch4_window7_224.ms_in22k', 'swin_tiny_patch4_window7_224.ms_in22k_ft_in1k', 'swinv2_base_window8_256.ms_in1k', 'swinv2_base_window12_192.ms_in22k', 'swinv2_base_window12to16_192to256.ms_in22k_ft_in1k', 'swinv2_base_window12to24_192to384.ms_in22k_ft_in1k', 'swinv2_base_window16_256.ms_in1k', 'swinv2_cr_small_224.sw_in1k', 'swinv2_cr_small_ns_224.sw_in1k', 'swinv2_cr_tiny_ns_224.sw_in1k', 'swinv2_large_window12_192.ms_in22k', 'swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', 'swinv2_large_window12to24_192to384.ms_in22k_ft_in1k', 'swinv2_small_window8_256.ms_in1k', 'swinv2_small_window16_256.ms_in1k', 'swinv2_tiny_window8_256.ms_in1k', 'swinv2_tiny_window16_256.ms_in1k', 'tf_efficientnet_b0.aa_in1k', 'tf_efficientnet_b0.ap_in1k', 'tf_efficientnet_b0.in1k', 'tf_efficientnet_b0.ns_jft_in1k', 'tf_efficientnet_b1.aa_in1k', 'tf_efficientnet_b1.ap_in1k', 'tf_efficientnet_b1.in1k', 'tf_efficientnet_b1.ns_jft_in1k', 'tf_efficientnet_b2.aa_in1k', 'tf_efficientnet_b2.ap_in1k', 'tf_efficientnet_b2.in1k', 'tf_efficientnet_b2.ns_jft_in1k', 'tf_efficientnet_b3.aa_in1k', 'tf_efficientnet_b3.ap_in1k', 'tf_efficientnet_b3.in1k', 'tf_efficientnet_b3.ns_jft_in1k', 'tf_efficientnet_b4.aa_in1k', 'tf_efficientnet_b4.ap_in1k', 'tf_efficientnet_b4.in1k', 'tf_efficientnet_b4.ns_jft_in1k', 'tf_efficientnet_b5.aa_in1k', 'tf_efficientnet_b5.ap_in1k', 'tf_efficientnet_b5.in1k', 'tf_efficientnet_b5.ns_jft_in1k', 'tf_efficientnet_b5.ra_in1k', 'tf_efficientnet_b6.aa_in1k', 'tf_efficientnet_b6.ap_in1k', 'tf_efficientnet_b6.ns_jft_in1k', 'tf_efficientnet_b7.aa_in1k', 'tf_efficientnet_b7.ap_in1k', 'tf_efficientnet_b7.ns_jft_in1k', 'tf_efficientnet_b7.ra_in1k', 'tf_efficientnet_b8.ap_in1k', 'tf_efficientnet_b8.ra_in1k', 'tf_efficientnet_cc_b0_4e.in1k', 'tf_efficientnet_cc_b0_8e.in1k', 'tf_efficientnet_cc_b1_8e.in1k', 'tf_efficientnet_el.in1k', 'tf_efficientnet_em.in1k', 'tf_efficientnet_es.in1k', 'tf_efficientnet_l2.ns_jft_in1k', 'tf_efficientnet_l2.ns_jft_in1k_475', 'tf_efficientnet_lite0.in1k', 'tf_efficientnet_lite1.in1k', 'tf_efficientnet_lite2.in1k', 'tf_efficientnet_lite3.in1k', 'tf_efficientnet_lite4.in1k', 'tf_efficientnetv2_b0.in1k', 'tf_efficientnetv2_b1.in1k', 'tf_efficientnetv2_b2.in1k', 'tf_efficientnetv2_b3.in1k', 'tf_efficientnetv2_b3.in21k', 'tf_efficientnetv2_b3.in21k_ft_in1k', 'tf_efficientnetv2_l.in1k', 'tf_efficientnetv2_l.in21k', 'tf_efficientnetv2_l.in21k_ft_in1k', 'tf_efficientnetv2_m.in1k', 'tf_efficientnetv2_m.in21k', 'tf_efficientnetv2_m.in21k_ft_in1k', 'tf_efficientnetv2_s.in1k', 'tf_efficientnetv2_s.in21k', 'tf_efficientnetv2_s.in21k_ft_in1k', 'tf_efficientnetv2_xl.in21k', 'tf_efficientnetv2_xl.in21k_ft_in1k', 'tf_mixnet_l.in1k', 'tf_mixnet_m.in1k', 'tf_mixnet_s.in1k', 'tf_mobilenetv3_large_075.in1k', 'tf_mobilenetv3_large_100.in1k', 'tf_mobilenetv3_large_minimal_100.in1k', 'tf_mobilenetv3_small_075.in1k', 'tf_mobilenetv3_small_100.in1k', 'tf_mobilenetv3_small_minimal_100.in1k', 'tinynet_a.in1k', 'tinynet_b.in1k', 'tinynet_c.in1k', 'tinynet_d.in1k', 'tinynet_e.in1k', 'tnt_s_patch16_224', 'tresnet_l.miil_in1k', 'tresnet_l.miil_in1k_448', 'tresnet_m.miil_in1k', 'tresnet_m.miil_in1k_448', 'tresnet_m.miil_in21k', 'tresnet_m.miil_in21k_ft_in1k', 'tresnet_v2_l.miil_in21k', 'tresnet_v2_l.miil_in21k_ft_in1k', 'tresnet_xl.miil_in1k', 'tresnet_xl.miil_in1k_448', 'twins_pcpvt_base.in1k', 'twins_pcpvt_large.in1k', 'twins_pcpvt_small.in1k', 'twins_svt_base.in1k', 'twins_svt_large.in1k', 'twins_svt_small.in1k', 'vgg11.tv_in1k', 'vgg11_bn.tv_in1k', 'vgg13.tv_in1k', 'vgg13_bn.tv_in1k', 'vgg16.tv_in1k', 'vgg16_bn.tv_in1k', 'vgg19.tv_in1k', 'vgg19_bn.tv_in1k', 'visformer_small.in1k', 'visformer_tiny.in1k', 'vit_base_patch8_224.augreg2_in21k_ft_in1k', 'vit_base_patch8_224.augreg_in21k', 'vit_base_patch8_224.augreg_in21k_ft_in1k', 'vit_base_patch8_224.dino', 'vit_base_patch14_dinov2.lvd142m', 'vit_base_patch16_224.augreg2_in21k_ft_in1k', 'vit_base_patch16_224.augreg_in1k', 'vit_base_patch16_224.augreg_in21k', 'vit_base_patch16_224.augreg_in21k_ft_in1k', 'vit_base_patch16_224.dino', 'vit_base_patch16_224.mae', 'vit_base_patch16_224.orig_in21k_ft_in1k', 'vit_base_patch16_224.sam_in1k', 'vit_base_patch16_224_miil.in21k', 'vit_base_patch16_224_miil.in21k_ft_in1k', 'vit_base_patch16_384.augreg_in1k', 'vit_base_patch16_384.augreg_in21k_ft_in1k', 'vit_base_patch16_384.orig_in21k_ft_in1k', 'vit_base_patch16_clip_224.laion2b', 'vit_base_patch16_clip_224.laion2b_ft_in1k', 'vit_base_patch16_clip_224.laion2b_ft_in12k', 'vit_base_patch16_clip_224.laion2b_ft_in12k_in1k', 'vit_base_patch16_clip_224.openai', 'vit_base_patch16_clip_224.openai_ft_in1k', 'vit_base_patch16_clip_224.openai_ft_in12k', 'vit_base_patch16_clip_224.openai_ft_in12k_in1k', 'vit_base_patch16_clip_384.laion2b_ft_in1k', 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k', 'vit_base_patch16_clip_384.openai_ft_in1k', 'vit_base_patch16_clip_384.openai_ft_in12k_in1k', 'vit_base_patch16_rpn_224.sw_in1k', 'vit_base_patch32_224.augreg_in1k', 'vit_base_patch32_224.augreg_in21k', 'vit_base_patch32_224.augreg_in21k_ft_in1k', 'vit_base_patch32_224.sam_in1k', 'vit_base_patch32_384.augreg_in1k', 'vit_base_patch32_384.augreg_in21k_ft_in1k', 'vit_base_patch32_clip_224.laion2b', 'vit_base_patch32_clip_224.laion2b_ft_in1k', 'vit_base_patch32_clip_224.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_224.openai', 'vit_base_patch32_clip_224.openai_ft_in1k', 'vit_base_patch32_clip_384.laion2b_ft_in12k_in1k', 'vit_base_patch32_clip_384.openai_ft_in12k_in1k', 'vit_base_patch32_clip_448.laion2b_ft_in12k_in1k', 'vit_base_r50_s16_224.orig_in21k', 'vit_base_r50_s16_384.orig_in21k_ft_in1k', 'vit_giant_patch14_clip_224.laion2b', 'vit_giant_patch14_dinov2.lvd142m', 'vit_gigantic_patch14_clip_224.laion2b', 'vit_huge_patch14_224.mae', 'vit_huge_patch14_224.orig_in21k', 'vit_huge_patch14_clip_224.laion2b', 'vit_huge_patch14_clip_224.laion2b_ft_in1k', 'vit_huge_patch14_clip_224.laion2b_ft_in12k', 'vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k', 'vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_224.datacompxl', 'vit_large_patch14_clip_224.laion2b', 'vit_large_patch14_clip_224.laion2b_ft_in1k', 'vit_large_patch14_clip_224.laion2b_ft_in12k', 'vit_large_patch14_clip_224.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_224.openai', 'vit_large_patch14_clip_224.openai_ft_in1k', 'vit_large_patch14_clip_224.openai_ft_in12k', 'vit_large_patch14_clip_224.openai_ft_in12k_in1k', 'vit_large_patch14_clip_336.laion2b_ft_in1k', 'vit_large_patch14_clip_336.laion2b_ft_in12k_in1k', 'vit_large_patch14_clip_336.openai', 'vit_large_patch14_clip_336.openai_ft_in12k_in1k', 'vit_large_patch14_dinov2.lvd142m', 'vit_large_patch16_224.augreg_in21k', 'vit_large_patch16_224.augreg_in21k_ft_in1k', 'vit_large_patch16_224.mae', 'vit_large_patch16_384.augreg_in21k_ft_in1k', 'vit_large_patch32_224.orig_in21k', 'vit_large_patch32_384.orig_in21k_ft_in1k', 'vit_large_r50_s32_224.augreg_in21k', 'vit_large_r50_s32_224.augreg_in21k_ft_in1k', 'vit_large_r50_s32_384.augreg_in21k_ft_in1k', 'vit_medium_patch16_gap_240.sw_in12k', 'vit_medium_patch16_gap_256.sw_in12k_ft_in1k', 'vit_medium_patch16_gap_384.sw_in12k_ft_in1k', 'vit_relpos_base_patch16_224.sw_in1k', 'vit_relpos_base_patch16_clsgap_224.sw_in1k', 'vit_relpos_base_patch32_plus_rpn_256.sw_in1k', 'vit_relpos_medium_patch16_224.sw_in1k', 'vit_relpos_medium_patch16_cls_224.sw_in1k', 'vit_relpos_medium_patch16_rpn_224.sw_in1k', 'vit_relpos_small_patch16_224.sw_in1k', 'vit_small_patch8_224.dino', 'vit_small_patch14_dinov2.lvd142m', 'vit_small_patch16_224.augreg_in1k', 'vit_small_patch16_224.augreg_in21k', 'vit_small_patch16_224.augreg_in21k_ft_in1k', 'vit_small_patch16_224.dino', 'vit_small_patch16_384.augreg_in1k', 'vit_small_patch16_384.augreg_in21k_ft_in1k', 'vit_small_patch32_224.augreg_in21k', 'vit_small_patch32_224.augreg_in21k_ft_in1k', 'vit_small_patch32_384.augreg_in21k_ft_in1k', 'vit_small_r26_s32_224.augreg_in21k', 'vit_small_r26_s32_224.augreg_in21k_ft_in1k', 'vit_small_r26_s32_384.augreg_in21k_ft_in1k', 'vit_srelpos_medium_patch16_224.sw_in1k', 'vit_srelpos_small_patch16_224.sw_in1k', 'vit_tiny_patch16_224.augreg_in21k', 'vit_tiny_patch16_224.augreg_in21k_ft_in1k', 'vit_tiny_patch16_384.augreg_in21k_ft_in1k', 'vit_tiny_r_s16_p8_224.augreg_in21k', 'vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k', 'vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k', 'volo_d1_224.sail_in1k', 'volo_d1_384.sail_in1k', 'volo_d2_224.sail_in1k', 'volo_d2_384.sail_in1k', 'volo_d3_224.sail_in1k', 'volo_d3_448.sail_in1k', 'volo_d4_224.sail_in1k', 'volo_d4_448.sail_in1k', 'volo_d5_224.sail_in1k', 'volo_d5_448.sail_in1k', 'volo_d5_512.sail_in1k', 'wide_resnet50_2.racm_in1k', 'wide_resnet50_2.tv2_in1k', 'wide_resnet50_2.tv_in1k', 'wide_resnet101_2.tv2_in1k', 'wide_resnet101_2.tv_in1k', 'xception41.tf_in1k', 'xception41p.ra3_in1k', 'xception65.ra3_in1k', 'xception65.tf_in1k', 'xception65p.ra3_in1k', 'xception71.tf_in1k', 'xcit_large_24_p8_224.fb_dist_in1k', 'xcit_large_24_p8_224.fb_in1k', 'xcit_large_24_p8_384.fb_dist_in1k', 'xcit_large_24_p16_224.fb_dist_in1k', 'xcit_large_24_p16_224.fb_in1k', 'xcit_large_24_p16_384.fb_dist_in1k', 'xcit_medium_24_p8_224.fb_dist_in1k', 'xcit_medium_24_p8_224.fb_in1k', 'xcit_medium_24_p8_384.fb_dist_in1k', 'xcit_medium_24_p16_224.fb_dist_in1k', 'xcit_medium_24_p16_224.fb_in1k', 'xcit_medium_24_p16_384.fb_dist_in1k', 'xcit_nano_12_p8_224.fb_dist_in1k', 'xcit_nano_12_p8_224.fb_in1k', 'xcit_nano_12_p8_384.fb_dist_in1k', 'xcit_nano_12_p16_224.fb_dist_in1k', 'xcit_nano_12_p16_224.fb_in1k', 'xcit_nano_12_p16_384.fb_dist_in1k', 'xcit_small_12_p8_224.fb_dist_in1k', 'xcit_small_12_p8_224.fb_in1k', 'xcit_small_12_p8_384.fb_dist_in1k', 'xcit_small_12_p16_224.fb_dist_in1k', 'xcit_small_12_p16_224.fb_in1k', 'xcit_small_12_p16_384.fb_dist_in1k', 'xcit_small_24_p8_224.fb_dist_in1k', 'xcit_small_24_p8_224.fb_in1k', 'xcit_small_24_p8_384.fb_dist_in1k', 'xcit_small_24_p16_224.fb_dist_in1k', 'xcit_small_24_p16_224.fb_in1k', 'xcit_small_24_p16_384.fb_dist_in1k', 'xcit_tiny_12_p8_224.fb_dist_in1k', 'xcit_tiny_12_p8_224.fb_in1k', 'xcit_tiny_12_p8_384.fb_dist_in1k', 'xcit_tiny_12_p16_224.fb_dist_in1k', 'xcit_tiny_12_p16_224.fb_in1k', 'xcit_tiny_12_p16_384.fb_dist_in1k', 'xcit_tiny_24_p8_224.fb_dist_in1k', 'xcit_tiny_24_p8_224.fb_in1k', 'xcit_tiny_24_p8_384.fb_dist_in1k', 'xcit_tiny_24_p16_224.fb_dist_in1k', 'xcit_tiny_24_p16_224.fb_in1k', 'xcit_tiny_24_p16_384.fb_dist_in1k']\n",
      "Help on class ClassificationTask in module terratorch.tasks.classification_tasks:\n",
      "\n",
      "class ClassificationTask(torchgeo.trainers.base.BaseTask)\n",
      " |  ClassificationTask(model_args: dict, model_factory: str, loss: str = 'ce', aux_heads: list[terratorch.models.model.AuxiliaryHead] | None = None, aux_loss: dict[str, float] | None = None, class_weights: list[float] | None = None, ignore_index: int | None = None, lr: float = 0.001, optimizer: str | None = None, optimizer_hparams: dict | None = None, scheduler: str | None = None, scheduler_hparams: dict | None = None, freeze_backbone: bool = False, freeze_decoder: bool = False, class_names: list[str] | None = None) -> None\n",
      " |  \n",
      " |  Classification Task that accepts models from a range of sources.\n",
      " |  \n",
      " |  This class is analog in functionality to class:ClassificationTask defined by torchgeo.\n",
      " |  However, it has some important differences:\n",
      " |      - Accepts the specification of a model factory\n",
      " |      - Logs metrics per class\n",
      " |      - Does not have any callbacks by default (TorchGeo tasks do early stopping by default)\n",
      " |      - Allows the setting of optimizers in the constructor\n",
      " |      - It provides mIoU with both Micro and Macro averaging\n",
      " |  \n",
      " |  .. note::\n",
      " |         * 'Micro' averaging suits overall performance evaluation but may not reflect\n",
      " |           minority class accuracy.\n",
      " |         * 'Macro' averaging gives equal weight to each class, useful\n",
      " |           for balanced performance assessment across imbalanced classes.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ClassificationTask\n",
      " |      torchgeo.trainers.base.BaseTask\n",
      " |      lightning.pytorch.core.module.LightningModule\n",
      " |      lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin\n",
      " |      lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin\n",
      " |      lightning.pytorch.core.hooks.ModelHooks\n",
      " |      lightning.pytorch.core.hooks.DataHooks\n",
      " |      lightning.pytorch.core.hooks.CheckpointHooks\n",
      " |      torch.nn.modules.module.Module\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_args: dict, model_factory: str, loss: str = 'ce', aux_heads: list[terratorch.models.model.AuxiliaryHead] | None = None, aux_loss: dict[str, float] | None = None, class_weights: list[float] | None = None, ignore_index: int | None = None, lr: float = 0.001, optimizer: str | None = None, optimizer_hparams: dict | None = None, scheduler: str | None = None, scheduler_hparams: dict | None = None, freeze_backbone: bool = False, freeze_decoder: bool = False, class_names: list[str] | None = None) -> None\n",
      " |      Constructor\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          Defaults to None.\n",
      " |          model_args (Dict): Arguments passed to the model factory.\n",
      " |          model_factory (str): ModelFactory class to be used to instantiate the model.\n",
      " |          loss (str, optional): Loss to be used. Currently, supports 'ce', 'jaccard' or 'focal' loss.\n",
      " |              Defaults to \"ce\".\n",
      " |          aux_loss (dict[str, float] | None, optional): Auxiliary loss weights.\n",
      " |              Should be a dictionary where the key is the name given to the loss\n",
      " |              and the value is the weight to be applied to that loss.\n",
      " |              The name of the loss should match the key in the dictionary output by the model's forward\n",
      " |              method containing that output. Defaults to None.\n",
      " |          class_weights (Union[list[float], None], optional): List of class weights to be applied to the loss.\n",
      " |          class_weights (list[float] | None, optional): List of class weights to be applied to the loss.\n",
      " |              Defaults to None.\n",
      " |          ignore_index (int | None, optional): Label to ignore in the loss computation. Defaults to None.\n",
      " |          lr (float, optional): Learning rate to be used. Defaults to 0.001.\n",
      " |          optimizer (str | None, optional): Name of optimizer class from torch.optim to be used.\n",
      " |              If None, will use Adam. Defaults to None. Overriden by config / cli specification through LightningCLI.\n",
      " |          optimizer_hparams (dict | None): Parameters to be passed for instantiation of the optimizer.\n",
      " |              Overriden by config / cli specification through LightningCLI.\n",
      " |          scheduler (str, optional): Name of Torch scheduler class from torch.optim.lr_scheduler\n",
      " |              to be used (e.g. ReduceLROnPlateau). Defaults to None.\n",
      " |              Overriden by config / cli specification through LightningCLI.\n",
      " |          scheduler_hparams (dict | None): Parameters to be passed for instantiation of the scheduler.\n",
      " |              Overriden by config / cli specification through LightningCLI.\n",
      " |          freeze_backbone (bool, optional): Whether to freeze the backbone. Defaults to False.\n",
      " |          freeze_decoder (bool, optional): Whether to freeze the decoder and segmentation head. Defaults to False.\n",
      " |          class_names (list[str] | None, optional): List of class names passed to metrics for better naming.\n",
      " |              Defaults to numeric ordering.\n",
      " |  \n",
      " |  configure_callbacks(self) -> list[lightning.pytorch.callbacks.callback.Callback]\n",
      " |      Configure model-specific callbacks. When the model gets attached, e.g., when ``.fit()`` or ``.test()`` gets\n",
      " |      called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainer's\n",
      " |      ``callbacks`` argument. If a callback returned here has the same type as one or several callbacks already\n",
      " |      present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will\n",
      " |      make sure :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callbacks run last.\n",
      " |      \n",
      " |      Return:\n",
      " |          A callback or a list of callbacks which will extend the list of callbacks in the Trainer.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def configure_callbacks(self):\n",
      " |              early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\")\n",
      " |              checkpoint = ModelCheckpoint(monitor=\"val_loss\")\n",
      " |              return [early_stop, checkpoint]\n",
      " |  \n",
      " |  configure_losses(self) -> None\n",
      " |      Initialize the loss criterion.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If *loss* is invalid.\n",
      " |  \n",
      " |  configure_metrics(self) -> None\n",
      " |      Initialize the performance metrics.\n",
      " |  \n",
      " |  configure_models(self) -> None\n",
      " |      Initialize the model.\n",
      " |  \n",
      " |  configure_optimizers(self) -> 'lightning.pytorch.utilities.types.OptimizerLRSchedulerConfig'\n",
      " |      Initialize the optimizer and learning rate scheduler.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Optimizer and learning rate scheduler.\n",
      " |  \n",
      " |  on_test_epoch_end(self) -> None\n",
      " |      Called in the test loop at the very end of the epoch.\n",
      " |  \n",
      " |  on_train_epoch_end(self) -> None\n",
      " |      Called in the training loop at the very end of the epoch.\n",
      " |      \n",
      " |      To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the\n",
      " |      :class:`~lightning.pytorch.LightningModule` and access them in this hook:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          class MyLightningModule(L.LightningModule):\n",
      " |              def __init__(self):\n",
      " |                  super().__init__()\n",
      " |                  self.training_step_outputs = []\n",
      " |      \n",
      " |              def training_step(self):\n",
      " |                  loss = ...\n",
      " |                  self.training_step_outputs.append(loss)\n",
      " |                  return loss\n",
      " |      \n",
      " |              def on_train_epoch_end(self):\n",
      " |                  # do something with all training_step outputs, for example:\n",
      " |                  epoch_mean = torch.stack(self.training_step_outputs).mean()\n",
      " |                  self.log(\"training_epoch_mean\", epoch_mean)\n",
      " |                  # free up the memory\n",
      " |                  self.training_step_outputs.clear()\n",
      " |  \n",
      " |  on_validation_epoch_end(self) -> None\n",
      " |      Called in the validation loop at the very end of the epoch.\n",
      " |  \n",
      " |  predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor\n",
      " |      Compute the predicted class probabilities.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The output of your DataLoader.\n",
      " |          batch_idx: Integer displaying index of this batch.\n",
      " |          dataloader_idx: Index of the current dataloader.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output predicted probabilities.\n",
      " |  \n",
      " |  test_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Compute the test loss and additional metrics.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The output of your DataLoader.\n",
      " |          batch_idx: Integer displaying index of this batch.\n",
      " |          dataloader_idx: Index of the current dataloader.\n",
      " |  \n",
      " |  training_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> torch.Tensor\n",
      " |      Compute the train loss and additional metrics.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The output of your DataLoader.\n",
      " |          batch_idx: Integer displaying index of this batch.\n",
      " |          dataloader_idx: Index of the current dataloader.\n",
      " |  \n",
      " |  validation_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Compute the validation loss and additional metrics.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The output of your DataLoader.\n",
      " |          batch_idx: Integer displaying index of this batch.\n",
      " |          dataloader_idx: Index of the current dataloader.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchgeo.trainers.base.BaseTask:\n",
      " |  \n",
      " |  forward(self, *args: Any, **kwargs: Any) -> Any\n",
      " |      Forward pass of the model.\n",
      " |      \n",
      " |      Args:\n",
      " |          args: Arguments to pass to model.\n",
      " |          kwargs: Keyword arguments to pass to model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torchgeo.trainers.base.BaseTask:\n",
      " |  \n",
      " |  mode = 'min'\n",
      " |  \n",
      " |  monitor = 'val_loss'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.core.module.LightningModule:\n",
      " |  \n",
      " |  __getstate__(self) -> Dict[str, Any]\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  all_gather(self, data: Union[torch.Tensor, Dict, List, Tuple], group: Optional[Any] = None, sync_grads: bool = False) -> Union[torch.Tensor, Dict, List, Tuple]\n",
      " |      Gather tensors or collections of tensors from multiple processes.\n",
      " |      \n",
      " |      This method needs to be called on all processes and the tensors need to have the same shape across all\n",
      " |      processes, otherwise your program will stall forever.\n",
      " |      \n",
      " |      Args:\n",
      " |          data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof.\n",
      " |          group: the process group to gather results from. Defaults to all processes (world)\n",
      " |          sync_grads: flag that allows users to synchronize gradients for the all_gather operation\n",
      " |      \n",
      " |      Return:\n",
      " |          A tensor of shape (world_size, batch, ...), or if the input was a collection\n",
      " |          the output will also be a collection with tensors of this shape.\n",
      " |  \n",
      " |  backward(self, loss: torch.Tensor, *args: Any, **kwargs: Any) -> None\n",
      " |      Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your own\n",
      " |      implementation if you need to.\n",
      " |      \n",
      " |      Args:\n",
      " |          loss: The loss tensor returned by :meth:`training_step`. If gradient accumulation is used, the loss here\n",
      " |              holds the normalized value (scaled by 1 / accumulation steps).\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def backward(self, loss):\n",
      " |              loss.backward()\n",
      " |  \n",
      " |  clip_gradients(self, optimizer: torch.optim.optimizer.Optimizer, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None) -> None\n",
      " |      Handles gradient clipping internally.\n",
      " |      \n",
      " |      Note:\n",
      " |          - Do not override this method. If you want to customize gradient clipping, consider using\n",
      " |            :meth:`configure_gradient_clipping` method.\n",
      " |          - For manual optimization (``self.automatic_optimization = False``), if you want to use\n",
      " |            gradient clipping, consider calling\n",
      " |            ``self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")``\n",
      " |            manually in the training step.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: Current optimizer being used.\n",
      " |          gradient_clip_val: The value at which to clip gradients.\n",
      " |          gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      " |              to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm.\n",
      " |  \n",
      " |  configure_gradient_clipping(self, optimizer: torch.optim.optimizer.Optimizer, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None) -> None\n",
      " |      Perform gradient clipping for the optimizer parameters. Called before :meth:`optimizer_step`.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: Current optimizer being used.\n",
      " |          gradient_clip_val: The value at which to clip gradients. By default, value passed in Trainer\n",
      " |              will be available here.\n",
      " |          gradient_clip_algorithm: The gradient clipping algorithm to use. By default, value\n",
      " |              passed in Trainer will be available here.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\n",
      " |              # Implement your own custom logic to clip gradients\n",
      " |              # You can call `self.clip_gradients` with your settings:\n",
      " |              self.clip_gradients(\n",
      " |                  optimizer,\n",
      " |                  gradient_clip_val=gradient_clip_val,\n",
      " |                  gradient_clip_algorithm=gradient_clip_algorithm\n",
      " |              )\n",
      " |  \n",
      " |  freeze(self) -> None\n",
      " |      Freeze all params for inference.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          model = MyLightningModule(...)\n",
      " |          model.freeze()\n",
      " |  \n",
      " |  load_from_checkpoint(cls, checkpoint_path: Union[str, pathlib.Path, IO], map_location: Union[torch.device, str, int, Callable[[torch.storage.UntypedStorage, str], Optional[torch.storage.UntypedStorage]], Dict[Union[torch.device, str, int], Union[torch.device, str, int]], NoneType] = None, hparams_file: Union[str, pathlib.Path, NoneType] = None, strict: Optional[bool] = None, **kwargs: Any) -> Self\n",
      " |      Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\n",
      " |      passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\n",
      " |      \n",
      " |      Any arguments specified through \\*\\*kwargs will override args stored in ``\"hyper_parameters\"``.\n",
      " |      \n",
      " |      Args:\n",
      " |          checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object\n",
      " |          map_location:\n",
      " |              If your checkpoint saved a GPU model and you now load on CPUs\n",
      " |              or a different number of GPUs, use this to map to the new setup.\n",
      " |              The behaviour is the same as in :func:`torch.load`.\n",
      " |          hparams_file: Optional path to a ``.yaml`` or ``.csv`` file with hierarchical structure\n",
      " |              as in this example::\n",
      " |      \n",
      " |                  drop_prob: 0.2\n",
      " |                  dataloader:\n",
      " |                      batch_size: 32\n",
      " |      \n",
      " |              You most likely won't need this since Lightning will always save the hyperparameters\n",
      " |              to the checkpoint.\n",
      " |              However, if your checkpoint weights don't have the hyperparameters saved,\n",
      " |              use this method to pass in a ``.yaml`` file with the hparams you'd like to use.\n",
      " |              These will be converted into a :class:`~dict` and passed into your\n",
      " |              :class:`LightningModule` for use.\n",
      " |      \n",
      " |              If your model's ``hparams`` argument is :class:`~argparse.Namespace`\n",
      " |              and ``.yaml`` file has hierarchical structure, you need to refactor your model to treat\n",
      " |              ``hparams`` as :class:`~dict`.\n",
      " |          strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys\n",
      " |              returned by this module's state dict. Defaults to ``True`` unless ``LightningModule.strict_loading`` is\n",
      " |              set, in which case it defaults to the value of ``LightningModule.strict_loading``.\n",
      " |          \\**kwargs: Any extra keyword args needed to init the model. Can also be used to override saved\n",
      " |              hyperparameter values.\n",
      " |      \n",
      " |      Return:\n",
      " |          :class:`LightningModule` instance with loaded weights and hyperparameters (if available).\n",
      " |      \n",
      " |      Note:\n",
      " |          ``load_from_checkpoint`` is a **class** method. You should use your :class:`LightningModule`\n",
      " |          **class** to call it instead of the :class:`LightningModule` instance, or a\n",
      " |          ``TypeError`` will be raised.\n",
      " |      \n",
      " |      Note:\n",
      " |          To ensure all layers can be loaded from the checkpoint, this function will call\n",
      " |          :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` directly after instantiating the\n",
      " |          model if this hook is overridden in your LightningModule. However, note that ``load_from_checkpoint`` does\n",
      " |          not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this\n",
      " |          case, consider loading through the Trainer via ``.fit(ckpt_path=...)``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          # load weights without mapping ...\n",
      " |          model = MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt')\n",
      " |      \n",
      " |          # or load weights mapping all weights from GPU 1 to GPU 0 ...\n",
      " |          map_location = {'cuda:1':'cuda:0'}\n",
      " |          model = MyLightningModule.load_from_checkpoint(\n",
      " |              'path/to/checkpoint.ckpt',\n",
      " |              map_location=map_location\n",
      " |          )\n",
      " |      \n",
      " |          # or load weights and hyperparameters from separate files.\n",
      " |          model = MyLightningModule.load_from_checkpoint(\n",
      " |              'path/to/checkpoint.ckpt',\n",
      " |              hparams_file='/path/to/hparams_file.yaml'\n",
      " |          )\n",
      " |      \n",
      " |          # override some of the params with new values\n",
      " |          model = MyLightningModule.load_from_checkpoint(\n",
      " |              PATH,\n",
      " |              num_layers=128,\n",
      " |              pretrained_ckpt_path=NEW_PATH,\n",
      " |          )\n",
      " |      \n",
      " |          # predict\n",
      " |          pretrained_model.eval()\n",
      " |          pretrained_model.freeze()\n",
      " |          y_hat = pretrained_model(x)\n",
      " |  \n",
      " |  log(self, name: str, value: Union[torchmetrics.metric.Metric, torch.Tensor, int, float], prog_bar: bool = False, logger: Optional[bool] = None, on_step: Optional[bool] = None, on_epoch: Optional[bool] = None, reduce_fx: Union[str, Callable] = 'mean', enable_graph: bool = False, sync_dist: bool = False, sync_dist_group: Optional[Any] = None, add_dataloader_idx: bool = True, batch_size: Optional[int] = None, metric_attribute: Optional[str] = None, rank_zero_only: bool = False) -> None\n",
      " |      Log a key, value pair.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          self.log('train_loss', loss)\n",
      " |      \n",
      " |      The default behavior per hook is documented here: :ref:`extensions/logging:Automatic Logging`.\n",
      " |      \n",
      " |      Args:\n",
      " |          name: key to log.\n",
      " |          value: value to log. Can be a ``float``, ``Tensor``, or a ``Metric``.\n",
      " |          prog_bar: if ``True`` logs to the progress bar.\n",
      " |          logger: if ``True`` logs to the logger.\n",
      " |          on_step: if ``True`` logs at this step. The default value is determined by the hook.\n",
      " |              See :ref:`extensions/logging:Automatic Logging` for details.\n",
      " |          on_epoch: if ``True`` logs epoch accumulated metrics. The default value is determined by the hook.\n",
      " |              See :ref:`extensions/logging:Automatic Logging` for details.\n",
      " |          reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\n",
      " |          enable_graph: if ``True``, will not auto detach the graph.\n",
      " |          sync_dist: if ``True``, reduces the metric across devices. Use with care as this may lead to a significant\n",
      " |              communication overhead.\n",
      " |          sync_dist_group: the DDP group to sync across.\n",
      " |          add_dataloader_idx: if ``True``, appends the index of the current dataloader to\n",
      " |              the name (when using multiple dataloaders). If False, user needs to give unique names for\n",
      " |              each dataloader to not mix the values.\n",
      " |          batch_size: Current batch_size. This will be directly inferred from the loaded batch,\n",
      " |              but for some data structures you might need to explicitly provide it.\n",
      " |          metric_attribute: To restore the metric state, Lightning requires the reference of the\n",
      " |              :class:`torchmetrics.Metric` in your model. This is found automatically if it is a model attribute.\n",
      " |          rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\n",
      " |              rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\n",
      " |              (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\n",
      " |              :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\n",
      " |  \n",
      " |  log_dict(self, dictionary: Union[Mapping[str, Union[torchmetrics.metric.Metric, torch.Tensor, int, float]], torchmetrics.collections.MetricCollection], prog_bar: bool = False, logger: Optional[bool] = None, on_step: Optional[bool] = None, on_epoch: Optional[bool] = None, reduce_fx: Union[str, Callable] = 'mean', enable_graph: bool = False, sync_dist: bool = False, sync_dist_group: Optional[Any] = None, add_dataloader_idx: bool = True, batch_size: Optional[int] = None, rank_zero_only: bool = False) -> None\n",
      " |      Log a dictionary of values at once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n}\n",
      " |          self.log_dict(values)\n",
      " |      \n",
      " |      Args:\n",
      " |          dictionary: key value pairs.\n",
      " |              The values can be a ``float``, ``Tensor``, ``Metric``, or ``MetricCollection``.\n",
      " |          prog_bar: if ``True`` logs to the progress base.\n",
      " |          logger: if ``True`` logs to the logger.\n",
      " |          on_step: if ``True`` logs at this step.\n",
      " |              ``None`` auto-logs for training_step but not validation/test_step.\n",
      " |              The default value is determined by the hook.\n",
      " |              See :ref:`extensions/logging:Automatic Logging` for details.\n",
      " |          on_epoch: if ``True`` logs epoch accumulated metrics.\n",
      " |              ``None`` auto-logs for val/test step but not ``training_step``.\n",
      " |              The default value is determined by the hook.\n",
      " |              See :ref:`extensions/logging:Automatic Logging` for details.\n",
      " |          reduce_fx: reduction function over step values for end of epoch. :meth:`torch.mean` by default.\n",
      " |          enable_graph: if ``True``, will not auto-detach the graph\n",
      " |          sync_dist: if ``True``, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant\n",
      " |              communication overhead.\n",
      " |          sync_dist_group: the ddp group to sync across.\n",
      " |          add_dataloader_idx: if ``True``, appends the index of the current dataloader to\n",
      " |              the name (when using multiple). If ``False``, user needs to give unique names for\n",
      " |              each dataloader to not mix values.\n",
      " |          batch_size: Current batch size. This will be directly inferred from the loaded batch,\n",
      " |              but some data structures might need to explicitly provide it.\n",
      " |          rank_zero_only: Tells Lightning if you are calling ``self.log`` from every process (default) or only from\n",
      " |              rank 0. If ``True``, you won't be able to use this metric as a monitor in callbacks\n",
      " |              (e.g., early stopping). Warning: Improper use can lead to deadlocks! See\n",
      " |              :ref:`Advanced Logging <visualize/logging_advanced:rank_zero_only>` for more details.\n",
      " |  \n",
      " |  lr_scheduler_step(self, scheduler: Union[torch.optim.lr_scheduler.LRScheduler, torch.optim.lr_scheduler.ReduceLROnPlateau], metric: Optional[Any]) -> None\n",
      " |      Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\n",
      " |      each scheduler. By default, Lightning calls ``step()`` and as shown in the example for each scheduler based on\n",
      " |      its ``interval``.\n",
      " |      \n",
      " |      Args:\n",
      " |          scheduler: Learning rate scheduler.\n",
      " |          metric: Value of the monitor used for schedulers like ``ReduceLROnPlateau``.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # DEFAULT\n",
      " |          def lr_scheduler_step(self, scheduler, metric):\n",
      " |              if metric is None:\n",
      " |                  scheduler.step()\n",
      " |              else:\n",
      " |                  scheduler.step(metric)\n",
      " |      \n",
      " |          # Alternative way to update schedulers if it requires an epoch value\n",
      " |          def lr_scheduler_step(self, scheduler, metric):\n",
      " |              scheduler.step(epoch=self.current_epoch)\n",
      " |  \n",
      " |  lr_schedulers(self) -> Union[NoneType, List[Union[lightning.fabric.utilities.types.LRScheduler, lightning.fabric.utilities.types.ReduceLROnPlateau]], lightning.fabric.utilities.types.LRScheduler, lightning.fabric.utilities.types.ReduceLROnPlateau]\n",
      " |      Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A single scheduler, or a list of schedulers in case multiple ones are present, or ``None`` if no\n",
      " |          schedulers were returned in :meth:`~lightning.pytorch.core.LightningModule.configure_optimizers`.\n",
      " |  \n",
      " |  manual_backward(self, loss: torch.Tensor, *args: Any, **kwargs: Any) -> None\n",
      " |      Call this directly from your :meth:`training_step` when doing optimizations manually. By using this,\n",
      " |      Lightning can ensure that all the proper scaling gets applied when using mixed precision.\n",
      " |      \n",
      " |      See :ref:`manual optimization<common/optimization:Manual optimization>` for more examples.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def training_step(...):\n",
      " |              opt = self.optimizers()\n",
      " |              loss = ...\n",
      " |              opt.zero_grad()\n",
      " |              # automatically applies scaling, etc...\n",
      " |              self.manual_backward(loss)\n",
      " |              opt.step()\n",
      " |      \n",
      " |      Args:\n",
      " |          loss: The tensor on which to compute gradients. Must have a graph attached.\n",
      " |          *args: Additional positional arguments to be forwarded to :meth:`~torch.Tensor.backward`\n",
      " |          **kwargs: Additional keyword arguments to be forwarded to :meth:`~torch.Tensor.backward`\n",
      " |  \n",
      " |  optimizer_step(self, epoch: int, batch_idx: int, optimizer: Union[torch.optim.optimizer.Optimizer, lightning.pytorch.core.optimizer.LightningOptimizer], optimizer_closure: Optional[Callable[[], Any]] = None) -> None\n",
      " |      Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\n",
      " |      the optimizer.\n",
      " |      \n",
      " |      By default, Lightning calls ``step()`` and ``zero_grad()`` as shown in the example.\n",
      " |      This method (and ``zero_grad()``) won't be called during the accumulation phase when\n",
      " |      ``Trainer(accumulate_grad_batches != 1)``. Overriding this hook has no benefit with manual optimization.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Current epoch\n",
      " |          batch_idx: Index of current batch\n",
      " |          optimizer: A PyTorch optimizer\n",
      " |          optimizer_closure: The optimizer closure. This closure must be executed as it includes the\n",
      " |              calls to ``training_step()``, ``optimizer.zero_grad()``, and ``backward()``.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # DEFAULT\n",
      " |          def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
      " |              optimizer.step(closure=optimizer_closure)\n",
      " |      \n",
      " |          # Learning rate warm-up\n",
      " |          def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
      " |              # update params\n",
      " |              optimizer.step(closure=optimizer_closure)\n",
      " |      \n",
      " |              # manually warm up lr without a scheduler\n",
      " |              if self.trainer.global_step < 500:\n",
      " |                  lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0)\n",
      " |                  for pg in optimizer.param_groups:\n",
      " |                      pg[\"lr\"] = lr_scale * self.learning_rate\n",
      " |  \n",
      " |  optimizer_zero_grad(self, epoch: int, batch_idx: int, optimizer: torch.optim.optimizer.Optimizer) -> None\n",
      " |      Override this method to change the default behaviour of ``optimizer.zero_grad()``.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Current epoch\n",
      " |          batch_idx: Index of current batch\n",
      " |          optimizer: A PyTorch optimizer\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # DEFAULT\n",
      " |          def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\n",
      " |              optimizer.zero_grad()\n",
      " |      \n",
      " |          # Set gradients to `None` instead of zero to improve performance (not required on `torch>=2.0.0`).\n",
      " |          def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\n",
      " |              optimizer.zero_grad(set_to_none=True)\n",
      " |      \n",
      " |      See :meth:`torch.optim.Optimizer.zero_grad` for the explanation of the above example.\n",
      " |  \n",
      " |  optimizers(self, use_pl_optimizer: bool = True) -> Union[torch.optim.optimizer.Optimizer, lightning.pytorch.core.optimizer.LightningOptimizer, lightning.fabric.wrappers._FabricOptimizer, List[torch.optim.optimizer.Optimizer], List[lightning.pytorch.core.optimizer.LightningOptimizer], List[lightning.fabric.wrappers._FabricOptimizer]]\n",
      " |      Returns the optimizer(s) that are being used during training. Useful for manual optimization.\n",
      " |      \n",
      " |      Args:\n",
      " |          use_pl_optimizer: If ``True``, will wrap the optimizer(s) in a\n",
      " |              :class:`~lightning.pytorch.core.optimizer.LightningOptimizer` for automatic handling of precision,\n",
      " |              profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the\n",
      " |              ``step`` method and custom optimizers that don't have this method are not supported.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A single optimizer, or a list of optimizers in case multiple ones are present.\n",
      " |  \n",
      " |  print(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Prints only from process 0. Use this in any distributed mode to log only once.\n",
      " |      \n",
      " |      Args:\n",
      " |          *args: The thing to print. The same as for Python's built-in print function.\n",
      " |          **kwargs: The same as for Python's built-in print function.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def forward(self, x):\n",
      " |              self.print(x, 'in forward')\n",
      " |  \n",
      " |  to_onnx(self, file_path: Union[str, pathlib.Path], input_sample: Optional[Any] = None, **kwargs: Any) -> None\n",
      " |      Saves the model in ONNX format.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: The path of the file the onnx model should be saved to.\n",
      " |          input_sample: An input for tracing. Default: None (Use self.example_input_array)\n",
      " |          **kwargs: Will be passed to torch.onnx.export function.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          class SimpleModel(LightningModule):\n",
      " |              def __init__(self):\n",
      " |                  super().__init__()\n",
      " |                  self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n",
      " |      \n",
      " |              def forward(self, x):\n",
      " |                  return torch.relu(self.l1(x.view(x.size(0), -1)\n",
      " |      \n",
      " |          model = SimpleModel()\n",
      " |          input_sample = torch.randn(1, 64)\n",
      " |          model.to_onnx(\"export.onnx\", input_sample, export_params=True)\n",
      " |  \n",
      " |  to_torchscript(self, file_path: Union[str, pathlib.Path, NoneType] = None, method: Optional[str] = 'script', example_inputs: Optional[Any] = None, **kwargs: Any) -> Union[torch.ScriptModule, Dict[str, torch.ScriptModule]]\n",
      " |      By default compiles the whole model to a :class:`~torch.jit.ScriptModule`. If you want to use tracing,\n",
      " |      please provided the argument ``method='trace'`` and make sure that either the `example_inputs` argument is\n",
      " |      provided, or the model has :attr:`example_input_array` set. If you would like to customize the modules that are\n",
      " |      scripted you should override this method. In case you want to return multiple modules, we recommend using a\n",
      " |      dictionary.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path where to save the torchscript. Default: None (no file saved).\n",
      " |          method: Whether to use TorchScript's script or trace method. Default: 'script'\n",
      " |          example_inputs: An input to be used to do tracing when method is set to 'trace'.\n",
      " |            Default: None (uses :attr:`example_input_array`)\n",
      " |          **kwargs: Additional arguments that will be passed to the :func:`torch.jit.script` or\n",
      " |            :func:`torch.jit.trace` function.\n",
      " |      \n",
      " |      Note:\n",
      " |          - Requires the implementation of the\n",
      " |            :meth:`~lightning.pytorch.core.LightningModule.forward` method.\n",
      " |          - The exported script will be set to evaluation mode.\n",
      " |          - It is recommended that you install the latest supported version of PyTorch\n",
      " |            to use this feature without limitations. See also the :mod:`torch.jit`\n",
      " |            documentation for supported features.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          class SimpleModel(LightningModule):\n",
      " |              def __init__(self):\n",
      " |                  super().__init__()\n",
      " |                  self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n",
      " |      \n",
      " |              def forward(self, x):\n",
      " |                  return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
      " |      \n",
      " |          model = SimpleModel()\n",
      " |          model.to_torchscript(file_path=\"model.pt\")\n",
      " |      \n",
      " |          torch.jit.save(model.to_torchscript(\n",
      " |              file_path=\"model_trace.pt\", method='trace', example_inputs=torch.randn(1, 64))\n",
      " |          )\n",
      " |      \n",
      " |      Return:\n",
      " |          This LightningModule as a torchscript, regardless of whether `file_path` is\n",
      " |          defined or not.\n",
      " |  \n",
      " |  toggle_optimizer(self, optimizer: Union[torch.optim.optimizer.Optimizer, lightning.pytorch.core.optimizer.LightningOptimizer]) -> None\n",
      " |      Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to\n",
      " |      prevent dangling gradients in multiple-optimizer setup.\n",
      " |      \n",
      " |      It works with :meth:`untoggle_optimizer` to make sure ``param_requires_grad_state`` is properly reset.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: The optimizer to toggle.\n",
      " |  \n",
      " |  unfreeze(self) -> None\n",
      " |      Unfreeze all parameters for training.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          model = MyLightningModule(...)\n",
      " |          model.unfreeze()\n",
      " |  \n",
      " |  untoggle_optimizer(self, optimizer: Union[torch.optim.optimizer.Optimizer, lightning.pytorch.core.optimizer.LightningOptimizer]) -> None\n",
      " |      Resets the state of required gradients that were toggled with :meth:`toggle_optimizer`.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: The optimizer to untoggle.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from lightning.pytorch.core.module.LightningModule:\n",
      " |  \n",
      " |  current_epoch\n",
      " |      The current epoch in the ``Trainer``, or 0 if not attached.\n",
      " |  \n",
      " |  global_rank\n",
      " |      The index of the current process across all nodes and devices.\n",
      " |  \n",
      " |  global_step\n",
      " |      Total training batches seen across all epochs.\n",
      " |      \n",
      " |      If no Trainer is attached, this propery is 0.\n",
      " |  \n",
      " |  local_rank\n",
      " |      The index of the current process within a single node.\n",
      " |  \n",
      " |  logger\n",
      " |      Reference to the logger object in the Trainer.\n",
      " |  \n",
      " |  loggers\n",
      " |      Reference to the list of loggers in the Trainer.\n",
      " |  \n",
      " |  on_gpu\n",
      " |      Returns ``True`` if this model is currently located on a GPU.\n",
      " |      \n",
      " |      Useful to set flags around the LightningModule for different CPU vs GPU behavior.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.pytorch.core.module.LightningModule:\n",
      " |  \n",
      " |  automatic_optimization\n",
      " |      If set to ``False`` you are responsible for calling ``.backward()``, ``.step()``, ``.zero_grad()``.\n",
      " |  \n",
      " |  example_input_array\n",
      " |      The example input array is a specification of what the module can consume in the :meth:`forward` method. The\n",
      " |      return type is interpreted as follows:\n",
      " |      \n",
      " |      -   Single tensor: It is assumed the model takes a single argument, i.e.,\n",
      " |          ``model.forward(model.example_input_array)``\n",
      " |      -   Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,\n",
      " |          ``model.forward(*model.example_input_array)``\n",
      " |      -   Dict: The input array represents named keyword arguments, i.e.,\n",
      " |          ``model.forward(**model.example_input_array)``\n",
      " |  \n",
      " |  fabric\n",
      " |  \n",
      " |  strict_loading\n",
      " |      Determines how Lightning loads this model using `.load_state_dict(..., strict=model.strict_loading)`.\n",
      " |  \n",
      " |  trainer\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from lightning.pytorch.core.module.LightningModule:\n",
      " |  \n",
      " |  CHECKPOINT_HYPER_PARAMS_KEY = 'hyper_parameters'\n",
      " |  \n",
      " |  CHECKPOINT_HYPER_PARAMS_NAME = 'hparams_name'\n",
      " |  \n",
      " |  CHECKPOINT_HYPER_PARAMS_TYPE = 'hparams_type'\n",
      " |  \n",
      " |  __jit_unused_properties__ = ['example_input_array', 'on_gpu', 'current...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin:\n",
      " |  \n",
      " |  cpu(self) -> Self\n",
      " |      See :meth:`torch.nn.Module.cpu`.\n",
      " |  \n",
      " |  cuda(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers\n",
      " |      different objects. So it should be called before constructing optimizer if the module will live on GPU while\n",
      " |      being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device: If specified, all parameters will be copied to that device. If `None`, the current CUDA device\n",
      " |              index will be used.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self) -> Self\n",
      " |      See :meth:`torch.nn.Module.double`.\n",
      " |  \n",
      " |  float(self) -> Self\n",
      " |      See :meth:`torch.nn.Module.float`.\n",
      " |  \n",
      " |  half(self) -> Self\n",
      " |      See :meth:`torch.nn.Module.half`.\n",
      " |  \n",
      " |  to(self, *args: Any, **kwargs: Any) -> Self\n",
      " |      See :meth:`torch.nn.Module.to`.\n",
      " |  \n",
      " |  type(self, dst_type: Union[str, torch.dtype]) -> Self\n",
      " |      See :meth:`torch.nn.Module.type`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin:\n",
      " |  \n",
      " |  device\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin:\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin:\n",
      " |  \n",
      " |  save_hyperparameters(self, *args: Any, ignore: Union[Sequence[str], str, NoneType] = None, frame: Optional[frame] = None, logger: bool = True) -> None\n",
      " |      Save arguments to ``hparams`` attribute.\n",
      " |      \n",
      " |      Args:\n",
      " |          args: single object of `dict`, `NameSpace` or `OmegaConf`\n",
      " |              or string names or arguments from class ``__init__``\n",
      " |          ignore: an argument name or a list of argument names from\n",
      " |              class ``__init__`` to be ignored\n",
      " |          frame: a frame object. Default is None\n",
      " |          logger: Whether to send the hyperparameters to the logger. Default: True\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin\n",
      " |          >>> class ManuallyArgsModel(HyperparametersMixin):\n",
      " |          ...     def __init__(self, arg1, arg2, arg3):\n",
      " |          ...         super().__init__()\n",
      " |          ...         # manually assign arguments\n",
      " |          ...         self.save_hyperparameters('arg1', 'arg3')\n",
      " |          ...     def forward(self, *args, **kwargs):\n",
      " |          ...         ...\n",
      " |          >>> model = ManuallyArgsModel(1, 'abc', 3.14)\n",
      " |          >>> model.hparams\n",
      " |          \"arg1\": 1\n",
      " |          \"arg3\": 3.14\n",
      " |      \n",
      " |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin\n",
      " |          >>> class AutomaticArgsModel(HyperparametersMixin):\n",
      " |          ...     def __init__(self, arg1, arg2, arg3):\n",
      " |          ...         super().__init__()\n",
      " |          ...         # equivalent automatic\n",
      " |          ...         self.save_hyperparameters()\n",
      " |          ...     def forward(self, *args, **kwargs):\n",
      " |          ...         ...\n",
      " |          >>> model = AutomaticArgsModel(1, 'abc', 3.14)\n",
      " |          >>> model.hparams\n",
      " |          \"arg1\": 1\n",
      " |          \"arg2\": abc\n",
      " |          \"arg3\": 3.14\n",
      " |      \n",
      " |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin\n",
      " |          >>> class SingleArgModel(HyperparametersMixin):\n",
      " |          ...     def __init__(self, params):\n",
      " |          ...         super().__init__()\n",
      " |          ...         # manually assign single argument\n",
      " |          ...         self.save_hyperparameters(params)\n",
      " |          ...     def forward(self, *args, **kwargs):\n",
      " |          ...         ...\n",
      " |          >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14))\n",
      " |          >>> model.hparams\n",
      " |          \"p1\": 1\n",
      " |          \"p2\": abc\n",
      " |          \"p3\": 3.14\n",
      " |      \n",
      " |          >>> from lightning.pytorch.core.mixins import HyperparametersMixin\n",
      " |          >>> class ManuallyArgsModel(HyperparametersMixin):\n",
      " |          ...     def __init__(self, arg1, arg2, arg3):\n",
      " |          ...         super().__init__()\n",
      " |          ...         # pass argument(s) to ignore as a string or in a list\n",
      " |          ...         self.save_hyperparameters(ignore='arg2')\n",
      " |          ...     def forward(self, *args, **kwargs):\n",
      " |          ...         ...\n",
      " |          >>> model = ManuallyArgsModel(1, 'abc', 3.14)\n",
      " |          >>> model.hparams\n",
      " |          \"arg1\": 1\n",
      " |          \"arg3\": 3.14\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin:\n",
      " |  \n",
      " |  hparams\n",
      " |      The collection of hyperparameters saved with :meth:`save_hyperparameters`. It is mutable by the user. For\n",
      " |      the frozen set of initial hyperparameters, use :attr:`hparams_initial`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Mutable hyperparameters dictionary\n",
      " |  \n",
      " |  hparams_initial\n",
      " |      The collection of hyperparameters saved with :meth:`save_hyperparameters`. These contents are read-only.\n",
      " |      Manual updates to the saved hyperparameters can instead be performed through :attr:`hparams`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          AttributeDict: immutable initial hyperparameters\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.core.hooks.ModelHooks:\n",
      " |  \n",
      " |  configure_model(self) -> None\n",
      " |      Hook to create modules in a strategy and precision aware context.\n",
      " |      \n",
      " |      This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where we'd like to shard\n",
      " |      the model instantly to save memory and initialization time.\n",
      " |      For non-sharded strategies, you can choose to override this hook or to initialize your model under the\n",
      " |      :meth:`~lightning.pytorch.trainer.trainer.Trainer.init_module` context manager.\n",
      " |      \n",
      " |      This hook is called during each of fit/val/test/predict stages in the same process, so ensure that\n",
      " |      implementation of this hook is **idempotent**, i.e., after the first time the hook is called, subsequent calls\n",
      " |      to it should be a no-op.\n",
      " |  \n",
      " |  configure_sharded_model(self) -> None\n",
      " |      Deprecated.\n",
      " |      \n",
      " |      Use :meth:`~lightning.pytorch.core.hooks.ModelHooks.configure_model` instead.\n",
      " |  \n",
      " |  on_after_backward(self) -> None\n",
      " |      Called after ``loss.backward()`` and before optimizers are stepped.\n",
      " |      \n",
      " |      Note:\n",
      " |          If using native AMP, the gradients will not be unscaled at this point.\n",
      " |          Use the ``on_before_optimizer_step`` if you need the unscaled gradients.\n",
      " |  \n",
      " |  on_before_backward(self, loss: torch.Tensor) -> None\n",
      " |      Called before ``loss.backward()``.\n",
      " |      \n",
      " |      Args:\n",
      " |          loss: Loss divided by number of batches for gradient accumulation and scaled if using AMP.\n",
      " |  \n",
      " |  on_before_optimizer_step(self, optimizer: torch.optim.optimizer.Optimizer) -> None\n",
      " |      Called before ``optimizer.step()``.\n",
      " |      \n",
      " |      If using gradient accumulation, the hook is called once the gradients have been accumulated.\n",
      " |      See: :paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`.\n",
      " |      \n",
      " |      If using AMP, the loss will be unscaled before calling this hook.\n",
      " |      See these `docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients>`__\n",
      " |      for more information on the scaling of gradients.\n",
      " |      \n",
      " |      If clipping gradients, the gradients will not have been clipped yet.\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: Current optimizer being used.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def on_before_optimizer_step(self, optimizer):\n",
      " |              # example to inspect gradient information in tensorboard\n",
      " |              if self.trainer.global_step % 25 == 0:  # don't make the tf file huge\n",
      " |                  for k, v in self.named_parameters():\n",
      " |                      self.logger.experiment.add_histogram(\n",
      " |                          tag=k, values=v.grad, global_step=self.trainer.global_step\n",
      " |                      )\n",
      " |  \n",
      " |  on_before_zero_grad(self, optimizer: torch.optim.optimizer.Optimizer) -> None\n",
      " |      Called after ``training_step()`` and before ``optimizer.zero_grad()``.\n",
      " |      \n",
      " |      Called in the training loop after taking an optimizer step and before zeroing grads.\n",
      " |      Good place to inspect weight information with weights updated.\n",
      " |      \n",
      " |      This is where it is called::\n",
      " |      \n",
      " |          for optimizer in optimizers:\n",
      " |              out = training_step(...)\n",
      " |      \n",
      " |              model.on_before_zero_grad(optimizer) # < ---- called here\n",
      " |              optimizer.zero_grad()\n",
      " |      \n",
      " |              backward()\n",
      " |      \n",
      " |      Args:\n",
      " |          optimizer: The optimizer for which grads should be zeroed.\n",
      " |  \n",
      " |  on_fit_end(self) -> None\n",
      " |      Called at the very end of fit.\n",
      " |      \n",
      " |      If on DDP it is called on every process\n",
      " |  \n",
      " |  on_fit_start(self) -> None\n",
      " |      Called at the very beginning of fit.\n",
      " |      \n",
      " |      If on DDP it is called on every process\n",
      " |  \n",
      " |  on_predict_batch_end(self, outputs: Optional[Any], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Called in the predict loop after the batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          outputs: The outputs of predict_step(x)\n",
      " |          batch: The batched data as it is returned by the prediction DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |          dataloader_idx: the index of the dataloader\n",
      " |  \n",
      " |  on_predict_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Called in the predict loop before anything happens for that batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The batched data as it is returned by the test DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |          dataloader_idx: the index of the dataloader\n",
      " |  \n",
      " |  on_predict_end(self) -> None\n",
      " |      Called at the end of predicting.\n",
      " |  \n",
      " |  on_predict_epoch_end(self) -> None\n",
      " |      Called at the end of predicting.\n",
      " |  \n",
      " |  on_predict_epoch_start(self) -> None\n",
      " |      Called at the beginning of predicting.\n",
      " |  \n",
      " |  on_predict_model_eval(self) -> None\n",
      " |      Called when the predict loop starts.\n",
      " |      \n",
      " |      The predict loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\n",
      " |      to change the behavior.\n",
      " |  \n",
      " |  on_predict_start(self) -> None\n",
      " |      Called at the beginning of predicting.\n",
      " |  \n",
      " |  on_test_batch_end(self, outputs: Union[torch.Tensor, Mapping[str, Any], NoneType], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Called in the test loop after the batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          outputs: The outputs of test_step(x)\n",
      " |          batch: The batched data as it is returned by the test DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |          dataloader_idx: the index of the dataloader\n",
      " |  \n",
      " |  on_test_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Called in the test loop before anything happens for that batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The batched data as it is returned by the test DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |          dataloader_idx: the index of the dataloader\n",
      " |  \n",
      " |  on_test_end(self) -> None\n",
      " |      Called at the end of testing.\n",
      " |  \n",
      " |  on_test_epoch_start(self) -> None\n",
      " |      Called in the test loop at the very beginning of the epoch.\n",
      " |  \n",
      " |  on_test_model_eval(self) -> None\n",
      " |      Called when the test loop starts.\n",
      " |      \n",
      " |      The test loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\n",
      " |      to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_train`.\n",
      " |  \n",
      " |  on_test_model_train(self) -> None\n",
      " |      Called when the test loop ends.\n",
      " |      \n",
      " |      The test loop by default restores the `training` mode of the LightningModule to what it was before\n",
      " |      starting testing. Override this hook to change the behavior. See also\n",
      " |      :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_test_model_eval`.\n",
      " |  \n",
      " |  on_test_start(self) -> None\n",
      " |      Called at the beginning of testing.\n",
      " |  \n",
      " |  on_train_batch_end(self, outputs: Union[torch.Tensor, Mapping[str, Any], NoneType], batch: Any, batch_idx: int) -> None\n",
      " |      Called in the training loop after the batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          outputs: The outputs of training_step(x)\n",
      " |          batch: The batched data as it is returned by the training DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |  \n",
      " |  on_train_batch_start(self, batch: Any, batch_idx: int) -> Optional[int]\n",
      " |      Called in the training loop before anything happens for that batch.\n",
      " |      \n",
      " |      If you return -1 here, you will skip training for the rest of the current epoch.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The batched data as it is returned by the training DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |  \n",
      " |  on_train_end(self) -> None\n",
      " |      Called at the end of training before logger experiment is closed.\n",
      " |  \n",
      " |  on_train_epoch_start(self) -> None\n",
      " |      Called in the training loop at the very beginning of the epoch.\n",
      " |  \n",
      " |  on_train_start(self) -> None\n",
      " |      Called at the beginning of training after sanity check.\n",
      " |  \n",
      " |  on_validation_batch_end(self, outputs: Union[torch.Tensor, Mapping[str, Any], NoneType], batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Called in the validation loop after the batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          outputs: The outputs of validation_step(x)\n",
      " |          batch: The batched data as it is returned by the validation DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |          dataloader_idx: the index of the dataloader\n",
      " |  \n",
      " |  on_validation_batch_start(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> None\n",
      " |      Called in the validation loop before anything happens for that batch.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: The batched data as it is returned by the validation DataLoader.\n",
      " |          batch_idx: the index of the batch\n",
      " |          dataloader_idx: the index of the dataloader\n",
      " |  \n",
      " |  on_validation_end(self) -> None\n",
      " |      Called at the end of validation.\n",
      " |  \n",
      " |  on_validation_epoch_start(self) -> None\n",
      " |      Called in the validation loop at the very beginning of the epoch.\n",
      " |  \n",
      " |  on_validation_model_eval(self) -> None\n",
      " |      Called when the validation loop starts.\n",
      " |      \n",
      " |      The validation loop by default calls ``.eval()`` on the LightningModule before it starts. Override this hook\n",
      " |      to change the behavior. See also :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_train`.\n",
      " |  \n",
      " |  on_validation_model_train(self) -> None\n",
      " |      Called when the validation loop ends.\n",
      " |      \n",
      " |      The validation loop by default restores the `training` mode of the LightningModule to what it was before\n",
      " |      starting validation. Override this hook to change the behavior. See also\n",
      " |      :meth:`~lightning.pytorch.core.hooks.ModelHooks.on_validation_model_eval`.\n",
      " |  \n",
      " |  on_validation_model_zero_grad(self) -> None\n",
      " |      Called by the training loop to release gradients before entering the validation loop.\n",
      " |  \n",
      " |  on_validation_start(self) -> None\n",
      " |      Called at the beginning of validation.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.core.hooks.DataHooks:\n",
      " |  \n",
      " |  on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any\n",
      " |      Override to alter or apply batch augmentations to your batch after it is transferred to the device.\n",
      " |      \n",
      " |      Note:\n",
      " |          To check the current state of execution of this hook you can use\n",
      " |          ``self.trainer.training/testing/validating/predicting`` so that you can\n",
      " |          add different logic as per your requirement.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: A batch of data that needs to be altered or augmented.\n",
      " |          dataloader_idx: The index of the dataloader to which the batch belongs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A batch of data\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def on_after_batch_transfer(self, batch, dataloader_idx):\n",
      " |              batch['x'] = gpu_transforms(batch['x'])\n",
      " |              return batch\n",
      " |      \n",
      " |      Raises:\n",
      " |          MisconfigurationException:\n",
      " |              If using IPUs, ``Trainer(accelerator='ipu')``.\n",
      " |      \n",
      " |      See Also:\n",
      " |          - :meth:`on_before_batch_transfer`\n",
      " |          - :meth:`transfer_batch_to_device`\n",
      " |  \n",
      " |  on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any\n",
      " |      Override to alter or apply batch augmentations to your batch before it is transferred to the device.\n",
      " |      \n",
      " |      Note:\n",
      " |          To check the current state of execution of this hook you can use\n",
      " |          ``self.trainer.training/testing/validating/predicting`` so that you can\n",
      " |          add different logic as per your requirement.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: A batch of data that needs to be altered or augmented.\n",
      " |          dataloader_idx: The index of the dataloader to which the batch belongs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A batch of data\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def on_before_batch_transfer(self, batch, dataloader_idx):\n",
      " |              batch['x'] = transforms(batch['x'])\n",
      " |              return batch\n",
      " |      \n",
      " |      See Also:\n",
      " |          - :meth:`on_after_batch_transfer`\n",
      " |          - :meth:`transfer_batch_to_device`\n",
      " |  \n",
      " |  predict_dataloader(self) -> Any\n",
      " |      An iterable or collection of iterables specifying prediction samples.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\n",
      " |      \n",
      " |      - :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`\n",
      " |      - :meth:`prepare_data`\n",
      " |      - :meth:`setup`\n",
      " |      \n",
      " |      Note:\n",
      " |          Lightning tries to add the correct sampler for distributed and arbitrary hardware\n",
      " |          There is no need to set it yourself.\n",
      " |      \n",
      " |      Return:\n",
      " |          A :class:`torch.utils.data.DataLoader` or a sequence of them specifying prediction samples.\n",
      " |  \n",
      " |  prepare_data(self) -> None\n",
      " |      Use this to download and prepare data. Downloading and saving data with multiple processes (distributed\n",
      " |      settings) will result in corrupted data. Lightning ensures this method is called only within a single process,\n",
      " |      so you can safely add your downloading logic within.\n",
      " |      \n",
      " |      .. warning:: DO NOT set state to the model (use ``setup`` instead)\n",
      " |          since this is NOT called on every device\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def prepare_data(self):\n",
      " |              # good\n",
      " |              download_data()\n",
      " |              tokenize()\n",
      " |              etc()\n",
      " |      \n",
      " |              # bad\n",
      " |              self.split = data_split\n",
      " |              self.some_state = some_other_state()\n",
      " |      \n",
      " |      In a distributed environment, ``prepare_data`` can be called in two ways\n",
      " |      (using :ref:`prepare_data_per_node<common/lightning_module:prepare_data_per_node>`)\n",
      " |      \n",
      " |      1. Once per node. This is the default and is only called on LOCAL_RANK=0.\n",
      " |      2. Once in total. Only called on GLOBAL_RANK=0.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          # DEFAULT\n",
      " |          # called once per node on LOCAL_RANK=0 of that node\n",
      " |          class LitDataModule(LightningDataModule):\n",
      " |              def __init__(self):\n",
      " |                  super().__init__()\n",
      " |                  self.prepare_data_per_node = True\n",
      " |      \n",
      " |      \n",
      " |          # call on GLOBAL_RANK=0 (great for shared file systems)\n",
      " |          class LitDataModule(LightningDataModule):\n",
      " |              def __init__(self):\n",
      " |                  super().__init__()\n",
      " |                  self.prepare_data_per_node = False\n",
      " |      \n",
      " |      This is called before requesting the dataloaders:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          model.prepare_data()\n",
      " |          initialize_distributed()\n",
      " |          model.setup(stage)\n",
      " |          model.train_dataloader()\n",
      " |          model.val_dataloader()\n",
      " |          model.test_dataloader()\n",
      " |          model.predict_dataloader()\n",
      " |  \n",
      " |  setup(self, stage: str) -> None\n",
      " |      Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you\n",
      " |      need to build models dynamically or adjust something about them. This hook is called on every process when\n",
      " |      using DDP.\n",
      " |      \n",
      " |      Args:\n",
      " |          stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          class LitModel(...):\n",
      " |              def __init__(self):\n",
      " |                  self.l1 = None\n",
      " |      \n",
      " |              def prepare_data(self):\n",
      " |                  download_data()\n",
      " |                  tokenize()\n",
      " |      \n",
      " |                  # don't do this\n",
      " |                  self.something = else\n",
      " |      \n",
      " |              def setup(self, stage):\n",
      " |                  data = load_data(...)\n",
      " |                  self.l1 = nn.Linear(28, data.num_classes)\n",
      " |  \n",
      " |  teardown(self, stage: str) -> None\n",
      " |      Called at the end of fit (train + validate), validate, test, or predict.\n",
      " |      \n",
      " |      Args:\n",
      " |          stage: either ``'fit'``, ``'validate'``, ``'test'``, or ``'predict'``\n",
      " |  \n",
      " |  test_dataloader(self) -> Any\n",
      " |      An iterable or collection of iterables specifying test samples.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      For data processing use the following pattern:\n",
      " |      \n",
      " |          - download in :meth:`prepare_data`\n",
      " |          - process and split in :meth:`setup`\n",
      " |      \n",
      " |      However, the above are only necessary for distributed processing.\n",
      " |      \n",
      " |      .. warning:: do not assign state in prepare_data\n",
      " |      \n",
      " |      \n",
      " |      - :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`\n",
      " |      - :meth:`prepare_data`\n",
      " |      - :meth:`setup`\n",
      " |      \n",
      " |      Note:\n",
      " |          Lightning tries to add the correct sampler for distributed and arbitrary hardware.\n",
      " |          There is no need to set it yourself.\n",
      " |      \n",
      " |      Note:\n",
      " |          If you don't need a test dataset and a :meth:`test_step`, you don't need to implement\n",
      " |          this method.\n",
      " |  \n",
      " |  train_dataloader(self) -> Any\n",
      " |      An iterable or collection of iterables specifying training samples.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      The dataloader you return will not be reloaded unless you set\n",
      " |      :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\n",
      " |      a positive integer.\n",
      " |      \n",
      " |      For data processing use the following pattern:\n",
      " |      \n",
      " |          - download in :meth:`prepare_data`\n",
      " |          - process and split in :meth:`setup`\n",
      " |      \n",
      " |      However, the above are only necessary for distributed processing.\n",
      " |      \n",
      " |      .. warning:: do not assign state in prepare_data\n",
      " |      \n",
      " |      - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\n",
      " |      - :meth:`prepare_data`\n",
      " |      - :meth:`setup`\n",
      " |      \n",
      " |      Note:\n",
      " |          Lightning tries to add the correct sampler for distributed and arbitrary hardware.\n",
      " |          There is no need to set it yourself.\n",
      " |  \n",
      " |  transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any\n",
      " |      Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\n",
      " |      structure.\n",
      " |      \n",
      " |      The data types listed below (and any arbitrary nesting of them) are supported out of the box:\n",
      " |      \n",
      " |      - :class:`torch.Tensor` or anything that implements `.to(...)`\n",
      " |      - :class:`list`\n",
      " |      - :class:`dict`\n",
      " |      - :class:`tuple`\n",
      " |      \n",
      " |      For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...).\n",
      " |      \n",
      " |      Note:\n",
      " |          This hook should only transfer the data and not modify it, nor should it move the data to\n",
      " |          any other device than the one passed in as argument (unless you know what you are doing).\n",
      " |          To check the current state of execution of this hook you can use\n",
      " |          ``self.trainer.training/testing/validating/predicting`` so that you can\n",
      " |          add different logic as per your requirement.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: A batch of data that needs to be transferred to a new device.\n",
      " |          device: The target device as defined in PyTorch.\n",
      " |          dataloader_idx: The index of the dataloader to which the batch belongs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A reference to the data on the new device.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def transfer_batch_to_device(self, batch, device, dataloader_idx):\n",
      " |              if isinstance(batch, CustomBatch):\n",
      " |                  # move all tensors in your custom data structure to the device\n",
      " |                  batch.samples = batch.samples.to(device)\n",
      " |                  batch.targets = batch.targets.to(device)\n",
      " |              elif dataloader_idx == 0:\n",
      " |                  # skip device transfer for the first dataloader or anything you wish\n",
      " |                  pass\n",
      " |              else:\n",
      " |                  batch = super().transfer_batch_to_device(batch, device, dataloader_idx)\n",
      " |              return batch\n",
      " |      \n",
      " |      Raises:\n",
      " |          MisconfigurationException:\n",
      " |              If using IPUs, ``Trainer(accelerator='ipu')``.\n",
      " |      \n",
      " |      See Also:\n",
      " |          - :meth:`move_data_to_device`\n",
      " |          - :meth:`apply_to_collection`\n",
      " |  \n",
      " |  val_dataloader(self) -> Any\n",
      " |      An iterable or collection of iterables specifying validation samples.\n",
      " |      \n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |      \n",
      " |      The dataloader you return will not be reloaded unless you set\n",
      " |      :paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs` to\n",
      " |      a positive integer.\n",
      " |      \n",
      " |      It's recommended that all data downloads and preparation happen in :meth:`prepare_data`.\n",
      " |      \n",
      " |      - :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`\n",
      " |      - :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`\n",
      " |      - :meth:`prepare_data`\n",
      " |      - :meth:`setup`\n",
      " |      \n",
      " |      Note:\n",
      " |          Lightning tries to add the correct sampler for distributed and arbitrary hardware\n",
      " |          There is no need to set it yourself.\n",
      " |      \n",
      " |      Note:\n",
      " |          If you don't need a validation dataset and a :meth:`validation_step`, you don't need to\n",
      " |          implement this method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.core.hooks.CheckpointHooks:\n",
      " |  \n",
      " |  on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None\n",
      " |      Called by Lightning to restore your model. If you saved something with :meth:`on_save_checkpoint` this is\n",
      " |      your chance to restore this.\n",
      " |      \n",
      " |      Args:\n",
      " |          checkpoint: Loaded checkpoint\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def on_load_checkpoint(self, checkpoint):\n",
      " |              # 99% of the time you don't need to implement this method\n",
      " |              self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save']\n",
      " |      \n",
      " |      Note:\n",
      " |          Lightning auto-restores global step, epoch, and train state including amp scaling.\n",
      " |          There is no need for you to restore anything regarding training.\n",
      " |  \n",
      " |  on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None\n",
      " |      Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to\n",
      " |      save.\n",
      " |      \n",
      " |      Args:\n",
      " |          checkpoint: The full checkpoint dictionary before it gets dumped to a file.\n",
      " |              Implementations of this hook can insert additional data into this dictionary.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          def on_save_checkpoint(self, checkpoint):\n",
      " |              # 99% of use cases you don't need to implement this method\n",
      " |              checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object\n",
      " |      \n",
      " |      Note:\n",
      " |          Lightning saves all aspects of training (epoch, global step, etc...)\n",
      " |          including amp scaling.\n",
      " |          There is no need for you to store anything about training.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      # On the return type:\n",
      " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
      " |      # This is done for better interop with various type checkers for the end users.\n",
      " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
      " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
      " |      # See full discussion on the problems with returning `Union` here\n",
      " |      # https://github.com/microsoft/pyright/issues/4213\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): whether to assign items in the state\n",
      " |              dictionary to their corresponding keys in the module instead\n",
      " |              of copying them inplace into the module's current parameters and buffers.\n",
      " |              When ``False``, the properties of the tensors in the current\n",
      " |              module are preserved while when ``True``, the properties of the\n",
      " |              Tensors in the state dict are preserved.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module\n",
      " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
      " |      respect to module outputs are computed. The hook should have the following\n",
      " |      signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Registers a post hook to be run after module's ``load_state_dict``\n",
      " |      is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      These hooks will be called with arguments: ``self``, ``prefix``,\n",
      " |      and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered\n",
      " |      hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device], recurse: bool = True) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Resets gradients of all model parameters. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(timm.list_pretrained())\n",
    "print(help(terratorch.tasks.ClassificationTask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Finetune Prithvi to act as a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import terratorch.models.backbones.prithvi_vit as prithvi_vit\n",
    "\n",
    "pretrained_bands = prithvi_vit.PRETRAINED_BANDS  # need to still select the correct bands\n",
    "\n",
    "VIT_UPERNET_NECK = [\n",
    "    {\"name\": \"SelectIndices\", \"indices\": [1, 2, 3, 4]},\n",
    "    {\"name\": \"ReshapeTokensToImage\"},\n",
    "    {\"name\": \"LearnedInterpolateToPyramidal\"},\n",
    "]\n",
    "\n",
    "\n",
    "model_args = {\n",
    "        \"backbone\": \"prithvi_vit_100\", # see timm.list_pretrained() \n",
    "        \"decoder\": \"UperNetDecoder\",\n",
    "        \"bands\": ('B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B010', 'B011', 'B012', 'B013'),\n",
    "        \"backbone_pretrained_cfg_overlay\":{\"file\": \"C:/Users/alhst/Documents/AI Master/Urban Computing/Project/Prithvi/Files/Prithvi_EO_V1_100M.pt\"}, # FUCK THE EO PEOPLE ON HUGGINGFACE FOR RENAMING THE FILE YOU PIECES OF SHIT\n",
    "        \"pretrained\":False,\n",
    "        \"num_classes\": 4,\n",
    "        \"necks\":  VIT_UPERNET_NECK\n",
    "}\n",
    "\n",
    "task = ClassificationTask(\n",
    "    model_args=model_args,\n",
    "    model_factory=\"PrithviModelFactory\",\n",
    "    # pretrained_cfg=dict(file=\"Prithvi_EO_V1_100M.pt\"),\n",
    "    loss=\"ce\",\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_hparams={\"weight_decay\": 0.05},\n",
    "    freeze_backbone=True,\n",
    "    class_names=[\"Fossil Hard coal\", \"Fossil Coal-derived gas\", \"Fossil Gas\", \"Fossil Oil\"]\n",
    ")\n",
    "\n",
    "# bins (classification)\n",
    "# sloop activation function eruit\n",
    "# investigate custom head??? Baseclass?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\alhst\\\\Documents\\\\AI Master\\\\Urban Computing\\\\Project\\\\Prithvi\\\\Files\\\\data\\\\images\\\\images\\\\training/300x300/positive\\\\0000__S2A-MSIL2A-ST20200104T110726-N0213-R094-T30UYV-20200104T122020.tif'\n",
      " 'c:\\\\Users\\\\alhst\\\\Documents\\\\AI Master\\\\Urban Computing\\\\Project\\\\Prithvi\\\\Files\\\\data\\\\images\\\\images\\\\training/300x300/positive\\\\0000__S2A-MSIL2A-ST20200206T111719-N0214-R137-T30UYV-20200206T122704.tif'\n",
      " 'c:\\\\Users\\\\alhst\\\\Documents\\\\AI Master\\\\Urban Computing\\\\Project\\\\Prithvi\\\\Files\\\\data\\\\images\\\\images\\\\training/300x300/positive\\\\0000__S2A-MSIL2A-ST20200317T111723-N0214-R137-T30UYV-20200317T123526.tif'\n",
      " ...\n",
      " 'c:\\\\Users\\\\alhst\\\\Documents\\\\AI Master\\\\Urban Computing\\\\Project\\\\Prithvi\\\\Files\\\\data\\\\images\\\\images\\\\training/300x300/positive\\\\0298__S2B-MSIL2A-ST20210408T101634-N0300-R022-T33UVS-20210408T132617.tif'\n",
      " 'c:\\\\Users\\\\alhst\\\\Documents\\\\AI Master\\\\Urban Computing\\\\Project\\\\Prithvi\\\\Files\\\\data\\\\images\\\\images\\\\training/300x300/positive\\\\0298__S2B-MSIL2A-ST20210428T101632-N0300-R022-T33UVS-20210428T125857.tif'\n",
      " 'c:\\\\Users\\\\alhst\\\\Documents\\\\AI Master\\\\Urban Computing\\\\Project\\\\Prithvi\\\\Files\\\\data\\\\images\\\\images\\\\training/300x300/positive\\\\0298__S2B-MSIL2A-ST20210607T101638-N0300-R022-T33UVS-20210607T163734.tif']\n"
     ]
    }
   ],
   "source": [
    "# Defining datamodules\n",
    "\n",
    "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
    "from dataset_original import create_dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, ConcatDataset, RandomSampler\n",
    "\n",
    "path = os.getcwd()  # current path\n",
    "datadir = os.path.join(path, \"data\\images\\images\")\n",
    "reg_file = os.path.join(path, \"data\\labels.csv\")\n",
    "seglabeldir = os.path.join(path, \"data\\segmentation_labels\\segmentation_labels\")\n",
    "\n",
    "reg_data = pd.read_csv(reg_file)\n",
    "# create dataset\n",
    "data_train_120x120 = create_dataset(datadir=os.path.join(datadir, 'training/120x120/'),\n",
    "                                    seglabeldir=os.path.join(seglabeldir, 'training/120x120/'),\n",
    "                                    reg_data=reg_data, mult=4, train=True, channels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
    "data_train_300x300 = create_dataset(datadir=os.path.join(datadir, 'training/300x300/'),\n",
    "                                    seglabeldir=os.path.join(seglabeldir, 'training/300x300/'),\n",
    "                                    reg_data=reg_data, mult=4, train=True, channels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], size=300)\n",
    "\n",
    "# data_train = ConcatDataset([data_train_120x120, data_train_300x300])\n",
    "data_train = data_train_300x300\n",
    "\n",
    "train_sampler = RandomSampler(data_train, replacement=True, num_samples=int(2 * len(data_train) / 3))\n",
    "\n",
    "# initialize data loaders\n",
    "train_dl = DataLoader(data_train, batch_size=32, num_workers=6,\n",
    "                        pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "print(data_train_300x300.imgfiles)  # printing information from MutiTaskDataset class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Constructing custom RasterDataset to transform our data to useable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "\n",
    "from torchgeo.datasets import RasterDataset, stack_samples, unbind_samples,GeoDataset\n",
    "from torchgeo.datasets.utils import download_url\n",
    "from torchgeo.samplers import RandomGeoSampler,GeoSampler,RandomBatchGeoSampler\n",
    "from torchgeo.datamodules import GeoDataModule\n",
    "\n",
    "import re\n",
    "from typing import cast\n",
    " \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "\n",
    "class Sentinel2(RasterDataset):\n",
    "    filename_glob = '*.tif'\n",
    "    #198_2019-01-31T10_06_36.654Z_1.tif\n",
    "    # filename_regex = r'^.{6}_(?P<date>\\d{8}T\\d{6})_(?P<band>B0[\\d])'\n",
    "    # date_format = '%Y%m%dT%H%M%S'\n",
    "    is_image = True\n",
    "    separate_files = True\n",
    "    all_bands = tuple([f'B0{i}' for i in range(1,14)])\n",
    "    rgb_bands = ('B04', 'B03', 'B02')\n",
    "    bands = ('B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B010', 'B011', 'B012', 'B013')\n",
    "    # bands = ('B01')\n",
    "\n",
    "    def plot(self, sample):\n",
    "        # Find the correct band index order\n",
    "        rgb_indices = []\n",
    "        for band in self.rgb_bands:\n",
    "            rgb_indices.append(self.all_bands.index(band))\n",
    "\n",
    "        # Reorder and rescale the image\n",
    "        image = sample['image'][rgb_indices].permute(1, 2, 0)\n",
    "        image = torch.clamp(image / 10000, min=0, max=1).numpy()\n",
    "\n",
    "        # Plot the image\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "\n",
    "        return fig\n",
    "    \n",
    "    def __getitem__(self,query):\n",
    "        #dit is de source code van RasterDataSet.__getitem__() tot waar ik sample['test'] toevoeg\n",
    "        #kan een work-around zijn?\n",
    "        hits = self.index.intersection(tuple(query), objects=True)\n",
    "        filepaths = cast(list[str], [hit.object for hit in hits])\n",
    "        labels = pd.read_csv(\"data/labels.csv\")\n",
    "\n",
    "\n",
    "        if not filepaths:\n",
    "            raise IndexError(\n",
    "                f'query: {query} not found in index with bounds: {self.bounds}'\n",
    "            )\n",
    "\n",
    "        if self.separate_files:\n",
    "            data_list: list[torch.Tensor] = []\n",
    "            # labels_list = []\n",
    "            filename_regex = re.compile(self.filename_regex, re.VERBOSE)\n",
    "            for band in self.bands:\n",
    "                band_filepaths = []\n",
    "                for filepath in filepaths:\n",
    "                    filename = os.path.basename(filepath)\n",
    "                    directory = os.path.dirname(filepath)\n",
    "                    match = re.match(filename_regex, filename)\n",
    "                    if match:\n",
    "                        if 'band' in match.groupdict():\n",
    "                            start = match.start('band')\n",
    "                            end = match.end('band')\n",
    "                            filename = filename[:start] + band + filename[end:]\n",
    "                    filepath = os.path.join(directory, filename)\n",
    "                    band_filepaths.append(filepath)\n",
    "                    # extracting labels\n",
    "                    image_data = labels[labels[\"filename\"] == filepath]\n",
    "                    # labels_list = torch.Tensor(image_data[\"fuel_type\"].values*13)  # transform to tensor for training, times 13 for each training band\n",
    "                    # labels_list = torch.Tensor(image_data[\"fuel_type\"])\n",
    "                # labels_list = torch.Tensor([0])  # for testing\n",
    "                # print(f'Nr. files in band {band}: {len(filepath)}')  # verify that each image is available in each band\n",
    "                data_list.append(self._merge_files(band_filepaths, query))\n",
    "            data = torch.cat(data_list)\n",
    "        else:\n",
    "            data = self._merge_files(filepaths, query, self.band_indexes)\n",
    "\n",
    "        sample = {'crs': self.crs, 'bounds': query}\n",
    "\n",
    "        labels_list = torch.Tensor([0])\n",
    "        data = data.to(self.dtype)\n",
    "        if self.is_image:\n",
    "            sample['image'] = data\n",
    "        else:\n",
    "            sample['mask'] = data\n",
    "\n",
    "        #hier evt eigen dingen toevoegen aan de sample\n",
    "        # sample['test'] = 1\n",
    "        sample['label'] = labels_list\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "\n",
    "        print(self.bands)  # 13 bands, \n",
    "        print(sample['image'].shape)  # 169 images of 120x120\n",
    "        print(sample['label'].shape)  # should have 13 labels? One for each band?\n",
    "        # print(sample['mask'].shape)  # commented out, becuase does not exist yet\n",
    "\n",
    "        return sample\n",
    "    \n",
    "root = os.path.join(os.getcwd(), \"data/images/images/training/120x120/positive\")\n",
    "dataset = Sentinel2(root)\n",
    "torch.manual_seed(1)    \n",
    "sampler = RandomGeoSampler(dataset,size=120,length=1) \n",
    "dataloader = DataLoader(dataset, sampler=sampler,collate_fn=stack_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule() got multiple values for keyword argument 'test_label_data_root'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 79\u001b[0m\n\u001b[0;32m     36\u001b[0m stds\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;241m1302.0157\u001b[39m, \u001b[38;5;241m1418.4988\u001b[39m, \u001b[38;5;241m1381.5366\u001b[39m, \u001b[38;5;241m1406.7112\u001b[39m, \u001b[38;5;241m1387.4155\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;241m1438.8479\u001b[39m, \u001b[38;5;241m1497.8815\u001b[39m, \u001b[38;5;241m1604.1998\u001b[39m, \u001b[38;5;241m1516.532\u001b[39m, \u001b[38;5;241m1827.3025\u001b[39m, \n\u001b[0;32m     39\u001b[0m     \u001b[38;5;241m1303.83\u001b[39m, \u001b[38;5;241m1189.9052\u001b[39m\n\u001b[0;32m     40\u001b[0m     ]  \u001b[38;5;66;03m# updated from dataset_multitask file \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# datamodule = GenericNonGeoSegmentationDataModule(\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#     batch_size,\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#     num_workers,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#     no_label_replace=-1,\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m GenericNonGeoSegmentationDataModule(\n\u001b[0;32m     80\u001b[0m     batch_size,\n\u001b[0;32m     81\u001b[0m     num_workers,\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;241m*\u001b[39mtrain_val_test,\n\u001b[0;32m     83\u001b[0m     img_grep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*_merged.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# img grep\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     label_grep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.mask.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# label grep\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     test_data_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_merged.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     86\u001b[0m     test_label_data_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mask.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     means\u001b[38;5;241m=\u001b[39mmeans,\n\u001b[0;32m     88\u001b[0m     stds\u001b[38;5;241m=\u001b[39mstds,\n\u001b[0;32m     89\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# num classes\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_val_test_labels,\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# if transforms are defined with Albumentations, you can pass them here\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# train_transform=train_transform,\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# val_transform=val_transform,\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# test_transform=test_transform,\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# edit the below for your usecase\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     dataset_bands\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     99\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mBLUE,\n\u001b[0;32m    100\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mGREEN,\n\u001b[0;32m    101\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mRED,\n\u001b[0;32m    102\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mNIR_NARROW,\n\u001b[0;32m    103\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mSWIR_1,\n\u001b[0;32m    104\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mSWIR_2,\n\u001b[0;32m    105\u001b[0m     ],\n\u001b[0;32m    106\u001b[0m     output_bands\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    107\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mBLUE,\n\u001b[0;32m    108\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mGREEN,\n\u001b[0;32m    109\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mRED,\n\u001b[0;32m    110\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mNIR_NARROW,\n\u001b[0;32m    111\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mSWIR_1,\n\u001b[0;32m    112\u001b[0m         HLSBands\u001b[38;5;241m.\u001b[39mSWIR_2,\n\u001b[0;32m    113\u001b[0m     ],\n\u001b[0;32m    114\u001b[0m     no_data_replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    115\u001b[0m     no_label_replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# we want to access some properties of the train dataset later on, so lets call setup here\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# if not, we would not need to\u001b[39;00m\n\u001b[0;32m    119\u001b[0m datamodule\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: terratorch.datamodules.generic_pixel_wise_data_module.GenericNonGeoSegmentationDataModule() got multiple values for keyword argument 'test_label_data_root'"
     ]
    }
   ],
   "source": [
    "from terratorch.datasets import HLSBands\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 0\n",
    "# train_val_test = [\n",
    "#     \"burn_scar_segmentation_toy/train_images\",\n",
    "#     \"burn_scar_segmentation_toy/val_images\",\n",
    "#     \"burn_scar_segmentation_toy/test_images\",\n",
    "# ]\n",
    "train_val_test = [\n",
    "    os.path.join(path, \"data\\images\\images\", 'training/300x300/'),\n",
    "    os.path.join(path, \"data\\images\\images\", 'validation/300x300/')\n",
    "]\n",
    "\n",
    "# train_val_test_labels = {\n",
    "#     \"train_label_data_root\": \"burn_scar_segmentation_toy/train_labels\",\n",
    "#     \"val_label_data_root\": \"burn_scar_segmentation_toy/val_labels\",\n",
    "#     \"test_label_data_root\": \"burn_scar_segmentation_toy/test_labels\",\n",
    "# }\n",
    "train_val_test_labels = {\n",
    "    \"train_label_data_root\": \"burn_scar_segmentation_toy/train_labels\",\n",
    "    \"val_label_data_root\": \"burn_scar_segmentation_toy/val_labels\",\n",
    "    \"test_label_data_root\": \"burn_scar_segmentation_toy/test_labels\",\n",
    "}  # still to edit\n",
    "\n",
    "\n",
    "\n",
    "# from https://github.com/NASA-IMPACT/hls-foundation-os/blob/main/configs/burn_scars.py\n",
    "\n",
    "means=[\n",
    "    960.97437, 1110.9012, 1250.0942, 1259.5178, 1500.98,\n",
    "    1989.6344, 2155.846, 2251.6265, 2272.9438, 2442.6206,\n",
    "    1914.3, 1512.0585\n",
    "    ]  # updated from dataset_multitask file\n",
    "\n",
    "stds=[\n",
    "    1302.0157, 1418.4988, 1381.5366, 1406.7112, 1387.4155,\n",
    "    1438.8479, 1497.8815, 1604.1998, 1516.532, 1827.3025, \n",
    "    1303.83, 1189.9052\n",
    "    ]  # updated from dataset_multitask file \n",
    "\n",
    "# datamodule = GenericNonGeoSegmentationDataModule(\n",
    "#     batch_size,\n",
    "#     num_workers,\n",
    "#     *train_val_test,\n",
    "#     \"*_merged.tif\", # img grep\n",
    "#     \"*.mask.tif\", # label grep\n",
    "#     means,\n",
    "#     stds,\n",
    "#     2, # num classes\n",
    "#     **train_val_test_labels,\n",
    "\n",
    "#     # if transforms are defined with Albumentations, you can pass them here\n",
    "#     # train_transform=train_transform,\n",
    "#     # val_transform=val_transform,\n",
    "#     # test_transform=test_transform,\n",
    "\n",
    "#     # edit the below for your usecase\n",
    "#     dataset_bands=[\n",
    "#         HLSBands.BLUE,\n",
    "#         HLSBands.GREEN,\n",
    "#         HLSBands.RED,\n",
    "#         HLSBands.NIR_NARROW,\n",
    "#         HLSBands.SWIR_1,\n",
    "#         HLSBands.SWIR_2,\n",
    "#     ],\n",
    "#     output_bands=[\n",
    "#         HLSBands.BLUE,\n",
    "#         HLSBands.GREEN,\n",
    "#         HLSBands.RED,\n",
    "#         HLSBands.NIR_NARROW,\n",
    "#         HLSBands.SWIR_1,\n",
    "#         HLSBands.SWIR_2,\n",
    "#     ],\n",
    "#     no_data_replace=0,\n",
    "#     no_label_replace=-1,\n",
    "# )\n",
    "\n",
    "datamodule = GenericNonGeoSegmentationDataModule(\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    *train_val_test,\n",
    "    img_grep=\"*_merged.tif\", # img grep\n",
    "    label_grep=\"*.mask.tif\", # label grep\n",
    "    test_data_root=\"_merged.tif\",\n",
    "    test_label_data_root=\".mask.tif\",\n",
    "    means=means,\n",
    "    stds=stds,\n",
    "    num_classes=2, # num classes\n",
    "    **train_val_test_labels,\n",
    "\n",
    "    # if transforms are defined with Albumentations, you can pass them here\n",
    "    # train_transform=train_transform,\n",
    "    # val_transform=val_transform,\n",
    "    # test_transform=test_transform,\n",
    "\n",
    "    # edit the below for your usecase\n",
    "    dataset_bands=[\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2,\n",
    "    ],\n",
    "    output_bands=[\n",
    "        HLSBands.BLUE,\n",
    "        HLSBands.GREEN,\n",
    "        HLSBands.RED,\n",
    "        HLSBands.NIR_NARROW,\n",
    "        HLSBands.SWIR_1,\n",
    "        HLSBands.SWIR_2,\n",
    "    ],\n",
    "    no_data_replace=0,\n",
    "    no_label_replace=-1,\n",
    ")\n",
    "# we want to access some properties of the train dataset later on, so lets call setup here\n",
    "# if not, we would not need to\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Defining Trainer and Custom Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\accelerator_connector.py:556: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name          </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type              </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model         │ ScalarOutputModel │  108 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ criterion     │ CrossEntropyLoss  │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ train_metrics │ MetricCollection  │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ val_metrics   │ MetricCollection  │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ test_metrics  │ MetricCollection  │      0 │\n",
       "└───┴───────────────┴───────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName         \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType             \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model         │ ScalarOutputModel │  108 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ criterion     │ CrossEntropyLoss  │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ train_metrics │ MetricCollection  │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ val_metrics   │ MetricCollection  │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ test_metrics  │ MetricCollection  │      0 │\n",
       "└───┴───────────────┴───────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 20.7 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 87.6 M                                                                                       \n",
       "<span style=\"font-weight: bold\">Total params</span>: 108 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 433                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 20.7 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 87.6 M                                                                                       \n",
       "\u001b[1mTotal params\u001b[0m: 108 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 433                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:4\n",
       "41: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the\n",
       "`num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:4\n",
       "41: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the\n",
       "`num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">B01\n",
       "</pre>\n"
      ],
      "text/plain": [
       "B01\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.Size([39, 120, 120])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.Size([39, 120, 120])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.Size([1])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.Size([1])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">B01\n",
       "</pre>\n"
      ],
      "text/plain": [
       "B01\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.Size([39, 120, 120])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.Size([39, 120, 120])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.Size([1])\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.Size([1])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\rich\\live.py:231: UserWarning: install \"ipywidgets\" for \n",
       "Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (6) to match target batch_size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m custom_datamodule \u001b[38;5;241m=\u001b[39m CustomGeoDataModule(\u001b[38;5;28mtype\u001b[39m(dataset), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# runtime error perhaps due to num_workers, could try 0 if commenting this out doesn't work (parallell resources)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# custom_datamodule = GeoDataModule(type(dataset), batch_size=1, patch_size=120, length=1, num_workers=6)  # previous module, doesn't work (gives \"split\" error)\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m _ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mfit(model\u001b[38;5;241m=\u001b[39mtask, train_dataloaders\u001b[38;5;241m=\u001b[39mcustom_datamodule)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    546\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    576\u001b[0m     ckpt_path,\n\u001b[0;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1057\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1060\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    395\u001b[0m )\n\u001b[1;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\terratorch\\tasks\\classification_tasks.py:230\u001b[0m, in \u001b[0;36mClassificationTask.validation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    228\u001b[0m y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    229\u001b[0m model_output: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m--> 230\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss_handler\u001b[38;5;241m.\u001b[39mcompute_loss(model_output, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_loss)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss_handler\u001b[38;5;241m.\u001b[39mlog_loss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog, loss_dict\u001b[38;5;241m=\u001b[39mloss, batch_size\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    232\u001b[0m y_hat_hard \u001b[38;5;241m=\u001b[39m to_class_prediction(model_output)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\terratorch\\tasks\\loss_handler.py:43\u001b[0m, in \u001b[0;36mLossHandler.compute_loss\u001b[1;34m(self, model_output, ground_truth, criterion, aux_loss_weights)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     21\u001b[0m     model_output: ModelOutput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     aux_loss_weights: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the loss for the mean decode head as well as other heads\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m            All other heads are returned with the same key as their name.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(model_output\u001b[38;5;241m.\u001b[39moutput, ground_truth, criterion)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_output\u001b[38;5;241m.\u001b[39mauxiliary_heads:\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\terratorch\\tasks\\loss_handler.py:72\u001b[0m, in \u001b[0;36mLossHandler._compute_loss\u001b[1;34m(self, y_hat, ground_truth, criterion)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_hat: Tensor, ground_truth: Tensor, criterion: Callable):\n\u001b[1;32m---> 72\u001b[0m     loss: Tensor \u001b[38;5;241m=\u001b[39m criterion(y_hat, ground_truth)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\alhst\\anaconda3\\envs\\terratorch\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (6) to match target batch_size (2)."
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, RichProgressBar\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchgeo.samplers import GridGeoSampler\n",
    "from torchgeo.datasets.splits import random_bbox_assignment\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=task.monitor, save_top_k=1, save_last=True)\n",
    "early_stopping_callback = EarlyStopping(monitor=task.monitor, min_delta=0.00, patience=20)\n",
    "logger = TensorBoardLogger(save_dir='output', name='tutorial')\n",
    "\n",
    "# You can also log directly to WandB\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "# wandb_logger = WandbLogger(log_model=\"all\") \n",
    "\n",
    "trainer = Trainer(\n",
    "    devices=1, # Number of GPUs. Interactive mode recommended with 1 device\n",
    "    precision=\"16-mixed\",\n",
    "    callbacks=[\n",
    "        RichProgressBar(),\n",
    "        checkpoint_callback,\n",
    "        early_stopping_callback,\n",
    "        LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "    ],\n",
    "    logger=logger,\n",
    "    max_epochs=1, # train only one epoch for demo\n",
    "    default_root_dir='output/test',\n",
    "    log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1\n",
    ")\n",
    "\n",
    "# for batch_idx, batch in enumerate(train_dl.keys()):\n",
    "#     # print(batch)\n",
    "#     print(batch)\n",
    "\n",
    "class CustomGeoDataModule(GeoDataModule):  # defining a custom datamodule to feed it to the trainer\n",
    "    def setup(self, stage: str) -> None:\n",
    "        \"\"\"Set up datasets.\n",
    "\n",
    "        Args:\n",
    "            stage: Either 'fit', 'validate', 'test', or 'predict'.\n",
    "        \"\"\"\n",
    "        self.dataset = self.dataset_class(**self.kwargs)\n",
    "        \n",
    "        generator = torch.Generator().manual_seed(0)\n",
    "        (\n",
    "            self.train_dataset,\n",
    "            self.val_dataset,\n",
    "            self.test_dataset,\n",
    "        ) = random_bbox_assignment(dataset, [0.6, 0.2, 0.2], generator)  # not sure what this does yet BUT IT IS VERY NECESSARY\n",
    "        \n",
    "        if stage in [\"fit\"]:\n",
    "            self.train_batch_sampler = RandomBatchGeoSampler(\n",
    "                self.train_dataset, self.patch_size, self.batch_size, self.length\n",
    "            )\n",
    "        if stage in [\"fit\", \"validate\"]:\n",
    "            self.val_sampler = GridGeoSampler(\n",
    "                self.val_dataset, self.patch_size, self.patch_size\n",
    "            )\n",
    "        if stage in [\"test\"]:\n",
    "            self.test_sampler = GridGeoSampler(\n",
    "                self.test_dataset, self.patch_size, self.patch_size\n",
    "            )\n",
    "\n",
    "custom_datamodule = CustomGeoDataModule(type(dataset), batch_size=2, patch_size=120, length=1)  # runtime error perhaps due to num_workers, could try 0 if commenting this out doesn't work (parallell resources)\n",
    "# custom_datamodule = GeoDataModule(type(dataset), batch_size=1, patch_size=120, length=1, num_workers=6)  # previous module, doesn't work (gives \"split\" error)\n",
    "_ = trainer.fit(model=task, train_dataloaders=custom_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The model was pretrained on bands {task._timm_module.pretrained_bands}.\\n The model is using bands {model._timm_module.model_bands}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "terratorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
